<!doctype html>
<html lang="en-US" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-zh-CN/Sensor/Grove/Grove_Sensors/AI-powered/cn-Train-Deploy-AI-Model-Grove-Vision-AI" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">Train and Deploy Your Own AI Model with Roboflow, YOLOv5, TensorFlow Lite | Seeed Studio Wiki</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://wiki.seeedstudio.com/cn/Train-Deploy-AI-Model-Grove-Vision-AI/"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Train and Deploy Your Own AI Model with Roboflow, YOLOv5, TensorFlow Lite | Seeed Studio Wiki"><meta data-rh="true" name="description" content="Train and Deploy Your Own AI Model with Roboflow, YOLOv5, TensorFlow Lite"><meta data-rh="true" property="og:description" content="Train and Deploy Your Own AI Model with Roboflow, YOLOv5, TensorFlow Lite"><meta data-rh="true" name="keywords" content="Sensor Vision_AI"><meta data-rh="true" property="og:image" content="https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png"><meta data-rh="true" name="twitter:image" content="https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png"><link data-rh="true" rel="icon" href="/img/S.png"><link data-rh="true" rel="canonical" href="https://wiki.seeedstudio.com/cn/Train-Deploy-AI-Model-Grove-Vision-AI/"><link data-rh="true" rel="alternate" href="https://wiki.seeedstudio.com/cn/Train-Deploy-AI-Model-Grove-Vision-AI/" hreflang="en-US"><link data-rh="true" rel="alternate" href="https://wiki.seeedstudio.com/cn/Train-Deploy-AI-Model-Grove-Vision-AI/" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Seeed Studio Wiki RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Seeed Studio Wiki Atom Feed">

<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n,g){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var m=t.getElementsByTagName(a)[0],r=t.createElement(a);r.async=!0,r.src="https://www.googletagmanager.com/gtm.js?id=GTM-M4JG2HVB",m.parentNode.insertBefore(r,m)}(window,document,"script","dataLayer")</script>


<link rel="icon" href="/img/S.png">
<link rel="manifest" href="/manifest.json">
<meta name="theme-color" content="rgb(37, 194, 160)">


<link rel="search" type="application/opensearchdescription+xml" title="Seeed Studio Wiki" href="/opensearch.xml">
<script src="https://viewer.altium.com/client/static/js/embed.js" async></script>
<script src="/js/custom.js" async></script><link rel="stylesheet" href="/assets/css/styles.c457b8a4.css">
<link rel="preload" href="/assets/js/runtime~main.fe3ab845.js" as="script">
<link rel="preload" href="/assets/js/main.7eadbfa1.js" as="script">
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M4JG2HVB" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:#013949;color:#FFFFFF" role="banner"><div class="content_knG7 announcementBarContent_xLdY">All planned updates to the wiki platform has been publicly availble. Your <a target="_blank" href="https://github.com/orgs/Seeed-Studio/projects/6?pane=issue&itemId=30957479">contributions</a> will be essential to us!</div></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Getting_Started/"><div class="navbar__logo"><img src="https://files.seeedstudio.com/wiki/wiki-platform/SeeedStudio.png" alt="Seeed Studio" class="themedImage_ToTc themedImage--light_HNdA navbar_logo_items"><img src="https://files.seeedstudio.com/wiki/wiki-platform/seeed_white_logo.png" alt="Seeed Studio" class="themedImage_ToTc themedImage--dark_i4oU navbar_logo_items"></div></a><div class="navbar__item dropdown dropdown--hoverable"><a class="navbar__link navbar_dorp_items" aria-haspopup="true" aria-expanded="false" role="button" href="/Getting_Started/">Getting Started</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/Sensor_Network/">Sensing and Connectivity</a></li><li><a class="dropdown__link" href="/Edge_Computing/">Edge Computing</a></li><li><a class="dropdown__link" href="/CloudnChain/">Cloud and Chain</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a class="navbar__link navbar_dorp_items" aria-haspopup="true" aria-expanded="false" role="button" href="/Getting_Started/">Topics</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/tinyml_topic/">TinyML</a></li><li><a class="dropdown__link" href="/home_assistant_topic/">Home Assistant</a></li><li><a class="dropdown__link" href="/open_source_topic/">Open Source</a></li><li><a class="dropdown__link" href="/edge_ai_topic/">Edge AI</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a class="navbar__link navbar_dorp_items" aria-haspopup="true" aria-expanded="false" role="button" href="/Getting_Started/">文/A</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/Getting_Started/">Seeed Studio Wiki Platform</a></li><li><a class="dropdown__link" href="/cn/Getting_Started/">矽递科技 Wiki 文档平台（测试）</a></li></ul></div></div><div class="navbar__items navbar__items--right"><a href="https://www.seeedstudio.com/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link navbar_doc_right_items">Bazaar 🛍️</a><a class="navbar__item navbar__link navbar_doc_right_items" href="/knowledgebase/">Help 🙋</a><a href="https://wiki-gpt.seeedstudio.com/chat" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link navbar_doc_right_items">Chat 🤖️</a><a href="https://seeed-studio.github.io/SenseCraft-Web-Toolkit/#/setup/process" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-SSCMA"></a><a href="https://github.com/Seeed-Studio/wiki-documents" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><main class="docMainContainer_gTbr docMainContainerEnhanced_Uz_u"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_z5aJ"><div class="docItemContainer_c0TR"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>训练并部署您自己的 AI 模型至 Grove - Vision AI</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="可升级为工业传感器">可升级为工业传感器<a href="#可升级为工业传感器" class="hash-link" aria-label="Direct link to 可升级为工业传感器" title="Direct link to 可升级为工业传感器">​</a></h2><p>借助 SenseCAP  <a href="https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html" target="_blank" rel="noopener noreferrer">S2110 控制器</a> 和 <a href="https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html" target="_blank" rel="noopener noreferrer">S2100 数据记录器</a>, 您可以轻松将Grove转换为LoRaWAN®传感器。Seeed不仅帮助您进行原型设计，还提供了通过SenseCAP系列稳健的<!-- -->[工业传感器]<!-- -->(<a href="https://www.seeedstudio.com/catalogsearch/result/?q=sensecap&amp;categories=SenseCAP&amp;application=Temperature%2FHumidity~Soil~Gas~Light~Weather~Water~Automation~Positioning~Machine" target="_blank" rel="noopener noreferrer">https://www.seeedstudio.com/catalogsearch/result/?q=sensecap&amp;categories=SenseCAP&amp;application=Temperature%2FHumidity~Soil~Gas~Light~Weather~Water~Automation~Positioning~Machine</a> Learning~Voice Recognition&amp;compatibility=SenseCAP)来扩展您的项目的可能性。</p><p>IP66外壳、蓝牙配置、全球LoRaWAN®网络兼容性、内置19Ah电池以及强大的APP支持使<!-- -->[SenseCAP S210x]<!-- -->(<a href="https://www.seeedstudio.com/catalogsearch/result/?q=S21&amp;categories=SenseCAP~LoRaWAN" target="_blank" rel="noopener noreferrer">https://www.seeedstudio.com/catalogsearch/result/?q=S21&amp;categories=SenseCAP~LoRaWAN</a> Device&amp;product_module=Device)成为工业应用的最佳选择。该系列包括用于土壤湿度、空气温度和湿度、光强度、CO2、EC以及8合1气象站的传感器。尝试最新的SenseCAP S210x，为您的下一个成功的工业项目增添动力。</p><table style="margin-left:auto;margin-right:auto"><tbody><tr><td colspan="4" bgcolor="#0e3c49" align="center"><font color="white" size="4"><strong>SenseCAP 工业传感器</strong></font></td></tr><tr><td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html" target="_blank" rel="noopener noreferrer"></a><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html" target="_blank" rel="noopener noreferrer"><img loading="lazy" width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2100.png" class="img_ev3q"></a></div></td><td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2101-LoRaWAN-Air-Temperature-and-Humidity-Sensor-p-5354.html" target="_blank" rel="noopener noreferrer"></a><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2101-LoRaWAN-Air-Temperature-and-Humidity-Sensor-p-5354.html" target="_blank" rel="noopener noreferrer"><img loading="lazy" width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2101&amp;S2103.png" class="img_ev3q"></a></div></td><td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2102-LoRaWAN-Light-Intensity-Sensor-p-5355.html" target="_blank" rel="noopener noreferrer"></a><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2102-LoRaWAN-Light-Intensity-Sensor-p-5355.html" target="_blank" rel="noopener noreferrer"><img loading="lazy" width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2102.png" class="img_ev3q"></a></div></td><td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2103-LoRaWAN-CO2-Temperature-and-Humidity-Sensor-p-5356.html" target="_blank" rel="noopener noreferrer"></a><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2103-LoRaWAN-CO2-Temperature-and-Humidity-Sensor-p-5356.html" target="_blank" rel="noopener noreferrer"><img loading="lazy" width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2101&amp;S2103.png" class="img_ev3q"></a></div></td></tr><tr><td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html" target="_blank" rel="noopener noreferrer"><strong>S2100 <br> 数据记录仪r</strong></a></td><td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2101-LoRaWAN-Air-Temperature-and-Humidity-Sensor-p-5354.html" target="_blank" rel="noopener noreferrer"><strong>S2101 <br> 空气温湿度</strong></a></td><td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2102-LoRaWAN-Light-Intensity-Sensor-p-5355.html" target="_blank" rel="noopener noreferrer"><strong>S2102 <br> 光照</strong></a></td><td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2103-LoRaWAN-CO2-Temperature-and-Humidity-Sensor-p-5356.html" target="_blank" rel="noopener noreferrer"><strong>S2103 <br> 空气温湿度和二氧化碳</strong></a></td></tr><tr><td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2104-LoRaWAN-Soil-Temperature-and-Moisture-Sensor-p-5357.html" target="_blank" rel="noopener noreferrer"></a><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2104-LoRaWAN-Soil-Temperature-and-Moisture-Sensor-p-5357.html" target="_blank" rel="noopener noreferrer"><img loading="lazy" width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2104.png" class="img_ev3q"></a></div></td><td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2105-LoRaWAN-Soil-Temperature-Moisture-and-EC-Sensor-p-5358.html" target="_blank" rel="noopener noreferrer"></a><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2105-LoRaWAN-Soil-Temperature-Moisture-and-EC-Sensor-p-5358.html" target="_blank" rel="noopener noreferrer"><img loading="lazy" width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2105.png" class="img_ev3q"></a></div></td><td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html" target="_blank" rel="noopener noreferrer"></a><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html" target="_blank" rel="noopener noreferrer"><img loading="lazy" width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2110.png" class="img_ev3q"></a></div></td><td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/sensecap-s2120-lorawan-8-in-1-weather-sensor-p-5436.html" target="_blank" rel="noopener noreferrer"></a><div align="center"><a href="https://www.seeedstudio.com/sensecap-s2120-lorawan-8-in-1-weather-sensor-p-5436.html" target="_blank" rel="noopener noreferrer"><img loading="lazy" width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2120.png" class="img_ev3q"></a></div></td></tr><tr><td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2104-LoRaWAN-Soil-Temperature-and-Moisture-Sensor-p-5357.html" target="_blank" rel="noopener noreferrer"><strong>S2104 <br> 土壤湿度和温度</strong></a></td><td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2105-LoRaWAN-Soil-Temperature-Moisture-and-EC-Sensor-p-5358.html" target="_blank" rel="noopener noreferrer"><strong>S2105 <br> 土壤温湿度和EC</strong></a></td><td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html" target="_blank" rel="noopener noreferrer"><strong>S2110 <br> LoRaWAN® 控制器</strong></a></td><td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/sensecap-s2120-lorawan-8-in-1-weather-sensor-p-5436.html" target="_blank" rel="noopener noreferrer"><strong>S2120 <br> 八合一气象站</strong></a></td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="概述">概述<a href="#概述" class="hash-link" aria-label="Direct link to 概述" title="Direct link to 概述">​</a></h2><p>在本wiki中，我们将教您如何为特定应用程序训练自己的AI模型，然后轻松将其部署到Grove - Vision AI模块上。让我们开始吧！</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="硬件介绍">硬件介绍<a href="#硬件介绍" class="hash-link" aria-label="Direct link to 硬件介绍" title="Direct link to 硬件介绍">​</a></h2><p>在本wiki中，我们主要使用Grove - Vision AI模块。因此，首先让我们熟悉一下硬件。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="grove---vision-ai模块">Grove - Vision AI模块<a href="#grove---vision-ai模块" class="hash-link" aria-label="Direct link to Grove - Vision AI模块" title="Direct link to Grove - Vision AI模块">​</a></h3><p><a href="https://www.seeedstudio.com/Grove-Vision-AI-Module-p-5457.html" target="_blank" rel="noopener noreferrer">Grove Vision AI模块</a>是一款拇指大小的AI摄像头，定制传感器，已经安装了用于人员检测和其他定制模型的ML算法。它可以在几分钟内轻松部署和显示，采用超低功耗模式工作，并提供两种信号传输方式和多个板载模块，所有这些都使它非常适合开始使用AI摄像头。</p><div align="center"><img loading="lazy" width="350" src="https://files.seeedstudio.com/wiki/Wio-Terminal-Developer-for-helium/camera.jpg" class="img_ev3q"></div><ul><li><h2 class="anchor anchorWithStickyNavbar_LWe7" id="软件介绍">软件介绍<a href="#软件介绍" class="hash-link" aria-label="Direct link to 软件介绍" title="Direct link to 软件介绍">​</a></h2><p>在本wiki中，我们将使用以下软件技术：</p><ul><li>Roboflow - 用于标注</li><li>YOLOv5 - 用于训练</li><li>TensorFlow Lite - 用于推理</li></ul></li></ul><div align="center"><img loading="lazy" width="600" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/57.png" class="img_ev3q"></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="什么是-roboflow">什么是 Roboflow？<a href="#什么是-roboflow" class="hash-link" aria-label="Direct link to 什么是 Roboflow？" title="Direct link to 什么是 Roboflow？">​</a></h3><p><a href="https://roboflow.com/" target="_blank" rel="noopener noreferrer">Roboflow</a> 是一个基于网络的标注工具。该工具允许您轻松地对所有图像进行标注，对这些图像进行进一步的处理，并将标记的数据集导出为不同的格式，如 YOLOV5 PyTorch、Pascal VOC 等！Roboflow 还提供了可供用户使用的公共数据集。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="什么是-yolov5">什么是 YOLOv5？<a href="#什么是-yolov5" class="hash-link" aria-label="Direct link to 什么是 YOLOv5？" title="Direct link to 什么是 YOLOv5？">​</a></h3><p>YOLO 是 “You Only Look Once” 的缩写。它是一种算法，可以实时检测和识别图像中的各种对象。Ultralytics 的 <a href="https://ultralytics.com/yolov5" target="_blank" rel="noopener noreferrer">YOLOv5</a> 是基于 PyTorch 框架的 YOLO 版本。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="什么是-tensorflow-lite">什么是 TensorFlow Lite？<a href="#什么是-tensorflow-lite" class="hash-link" aria-label="Direct link to 什么是 TensorFlow Lite？" title="Direct link to 什么是 TensorFlow Lite？">​</a></h3><p><a href="https://www.tensorflow.org/lite" target="_blank" rel="noopener noreferrer">TensorFlow Lite</a> 是一个开源、可用于产品的、跨平台的深度学习框架，它将 TensorFlow 中的预训练模型转换为可以针对速度或存储进行优化的特殊格式。特殊格式的模型可以部署在移动设备（如 Android 或 iOS）或基于 Linux 的嵌入式设备（如树莓派或微控制器）上，以在边缘进行推理。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="wiki-结构">Wiki 结构<a href="#wiki-结构" class="hash-link" aria-label="Direct link to Wiki 结构" title="Direct link to Wiki 结构">​</a></h2><p>本 wiki 将分为三个主要部分：</p><ol><li><a href="https://chat.openai.com/c/2e9a15cc-539c-4f18-b5ca-97525b204e78#jump1" target="_blank" rel="noopener noreferrer">使用公共数据集训练自己的 AI 模型</a></li><li><a href="https://chat.openai.com/c/2e9a15cc-539c-4f18-b5ca-97525b204e78#jump2" target="_blank" rel="noopener noreferrer">使用自己的数据集训练自己的 AI 模型</a></li><li><a href="https://chat.openai.com/c/2e9a15cc-539c-4f18-b5ca-97525b204e78#jump3" target="_blank" rel="noopener noreferrer">将训练好的 AI 模型部署到 Grove - Vision AI 模块</a></li></ol><p>第一部分将是构建自己的 AI 模型的最快方式，步骤最少。第二部分需要一些时间和精力来构建自己的 AI 模型，但它绝对值得。关于部署 AI 模型的第三部分可以在第一部分或第二部分之后进行。</p><p>因此，有两种方法可以遵循本 wiki：</p><ol><li>首先遵循 <a href="https://chat.openai.com/c/2e9a15cc-539c-4f18-b5ca-97525b204e78#jump1" target="_blank" rel="noopener noreferrer">第一部分</a>，然后遵循 <a href="https://chat.openai.com/c/2e9a15cc-539c-4f18-b5ca-97525b204e78#jump3" target="_blank" rel="noopener noreferrer">第三部分</a> - 快速跟进</li><li>首先遵循 <a href="https://chat.openai.com/c/2e9a15cc-539c-4f18-b5ca-97525b204e78#jump2" target="_blank" rel="noopener noreferrer">第二部分</a>，然后遵循 <a href="https://chat.openai.com/c/2e9a15cc-539c-4f18-b5ca-97525b204e78#jump3" target="_blank" rel="noopener noreferrer">第三部分</a> - 跟进较慢</li></ol><p>但我们建议首先采用第一种方法，然后转到第二种方法。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-1-使用公共数据集训练自己的-ai-模型"><span id="jump1">1. 1. 使用公共数据集训练自己的 AI 模型</span><a href="#1-1-使用公共数据集训练自己的-ai-模型" class="hash-link" aria-label="Direct link to 1-1-使用公共数据集训练自己的-ai-模型" title="Direct link to 1-1-使用公共数据集训练自己的-ai-模型">​</a></h2><p>对象检测项目的第一步是获取用于训练的数据。您可以下载公开可用的数据集，也可以创建自己的数据集！</p><p>但是，什么是开始进行对象检测的最快、最简单的方法呢？嗯...使用公共数据集可以节省您大量的时间，否则您将花费在收集数据和标注数据上。这些公共数据集已经被标注，让您有更多的时间专注于您的 AI 视觉应用。</p><ul><li><h3 class="anchor anchorWithStickyNavbar_LWe7" id="硬件准备">硬件准备<a href="#硬件准备" class="hash-link" aria-label="Direct link to 硬件准备" title="Direct link to 硬件准备">​</a></h3><ul><li>Grove - Vision AI 模块</li><li>USB Type-C 数据线</li><li>有互联网访问权限的 Windows / Linux / Mac</li></ul></li><li><h3 class="anchor anchorWithStickyNavbar_LWe7" id="软件准备">软件准备<a href="#软件准备" class="hash-link" aria-label="Direct link to 软件准备" title="Direct link to 软件准备">​</a></h3><ul><li>无需准备额外的软件</li></ul></li><li><h3 class="anchor anchorWithStickyNavbar_LWe7" id="使用公开可用的标注数据集">使用公开可用的标注数据集<a href="#使用公开可用的标注数据集" class="hash-link" aria-label="Direct link to 使用公开可用的标注数据集" title="Direct link to 使用公开可用的标注数据集">​</a></h3><p>您可以下载许多公开可用的数据集，例如 <a href="https://cocodataset.org/" target="_blank" rel="noopener noreferrer">COCO 数据集</a>、<a href="http://host.robots.ox.ac.uk/pascal/VOC" target="_blank" rel="noopener noreferrer">Pascal VOC 数据集</a> 等等。<a href="https://universe.roboflow.com/" target="_blank" rel="noopener noreferrer">Roboflow Universe</a> 是一个推荐的平台，它提供了广泛的数据集，拥有 <a href="https://blog.roboflow.com/computer-vision-datasets-and-apis" target="_blank" rel="noopener noreferrer">90,000+ 数据集，6600 万张图像</a>，可用于构建计算机视觉模型。此外，您还可以在 Google 上简单搜索 <strong>开源数据集</strong>，从众多可用的数据集中进行选择。</p><ul><li><strong>步骤 1.</strong> 访问 <a href="https://universe.roboflow.com/lakshantha-dissanayake/apple-detection-5z37o/dataset/1" target="_blank" rel="noopener noreferrer">此网址</a> 查看 Roboflow Universe 上公开可用的一个苹果检测数据集</li><li><strong>步骤 2.</strong> 单击 <strong>创建账户</strong> 创建 Roboflow 账户</li></ul></li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/53.png" class="img_ev3q"></div><ul><li><strong>步骤 3.</strong> 单击 <strong>下载</strong>，选择 <strong>YOLO v5 PyTorch</strong> 作为 <strong>格式</strong>，点击 <strong>显示下载代码</strong>，然后点击 <strong>继续</strong></li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/51.png" class="img_ev3q"></div><p>这将生成一个代码片段，我们稍后将在 Google Colab 训练中使用。所以请保持此窗口在后台打开。</p><div align="center"><img loading="lazy" width="700" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/52.png" class="img_ev3q"></div><ul><li><h3 class="anchor anchorWithStickyNavbar_LWe7" id="在-google-colab-上使用-yolov5-进行训练">在 Google Colab 上使用 YOLOv5 进行训练<a href="#在-google-colab-上使用-yolov5-进行训练" class="hash-link" aria-label="Direct link to 在 Google Colab 上使用 YOLOv5 进行训练" title="Direct link to 在 Google Colab 上使用 YOLOv5 进行训练">​</a></h3><p>在选择了公共数据集之后，我们需要对数据集进行训练。在这里，我们使用 Google Colaboratory 环境在云端进行训练。此外，我们在 Colab 中使用 Roboflow API 来轻松下载我们的数据集。</p><p>点击<a href="https://colab.research.google.com/gist/lakshanthad/b47a1d1a9b4fac43449948524de7d374/yolov5-training-for-sensecap-a1101.ipynb" target="_blank" rel="noopener noreferrer">此处</a>打开一个已经准备好的 Google Colab 工作区，按照工作区中提到的步骤逐步操作，并逐个运行代码单元。</p><p><strong>注意：</strong> 在 Google Colab 上，在 <strong>Step 4</strong> 下的代码单元中，您可以直接从 Roboflow 复制上面提到的代码片段。</p><p>它将会执行以下步骤：</p><ul><li>设置训练环境</li><li>下载数据集</li><li>执行训练</li><li>下载训练好的模型</li></ul></li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/18.png" class="img_ev3q"></div><p>对于包含 699 张图像的苹果检测数据集，在配备 NVIDIA Tesla T4 GPU 和 16GB GPU 内存的 Google Colab 上完成训练过程大约需要 7 分钟。</p><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/43.png" class="img_ev3q"></div><p>如果您跟随上述的 Colab 项目，您就会知道您可以一次加载 4 个模型到设备上。但是，请注意一次只能加载一个模型。这可以由用户指定，并将在本 Wiki 的后续部分进行解释。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="部署和推理">部署和推理<a href="#部署和推理" class="hash-link" aria-label="Direct link to 部署和推理" title="Direct link to 部署和推理">​</a></h2><p>如果您直接想跳到<strong>第3节</strong>，其中解释了如何将训练好的 AI 模型部署到 Grove - Vision AI 模块并进行推理，请<a href="https://chat.openai.com/c/2e9a15cc-539c-4f18-b5ca-97525b204e78#jump3" target="_blank" rel="noopener noreferrer">点击这里</a>。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-2-使用您自己的数据集训练自己的-ai-模型"><span id="jump2">2. 2. 使用您自己的数据集训练自己的 AI 模型</span><a href="#2-2-使用您自己的数据集训练自己的-ai-模型" class="hash-link" aria-label="Direct link to 2-2-使用您自己的数据集训练自己的-ai-模型" title="Direct link to 2-2-使用您自己的数据集训练自己的-ai-模型">​</a></h2><p>如果您想构建特定的目标检测项目，而公共数据集中没有您想要检测的对象，您可能希望构建自己的数据集。当您记录自己的数据集时，您必须确保覆盖对象的所有角度（360度），将对象放置在不同的环境中，不同的光照条件和不同的天气条件下。在记录自己的数据集后，您还必须对数据集中的图像进行注释。所有这些步骤将在本节中进行介绍。</p><p>尽管有不同的收集数据的方法，例如使用手机相机，但收集数据的最佳方法是使用 Grove - Vision AI 模块上的内置摄像头。这是因为当我们对 Grove - Vision AI 模块进行推断时，颜色、图像质量和其他细节都将相似，这使得整体检测更加准确。</p><ul><li><h3 class="anchor anchorWithStickyNavbar_LWe7" id="使用-roboflow-注释数据集">使用 Roboflow 注释数据集<a href="#使用-roboflow-注释数据集" class="hash-link" aria-label="Direct link to 使用 Roboflow 注释数据集" title="Direct link to 使用 Roboflow 注释数据集">​</a></h3><p>如果您使用自己的数据集，您将需要对数据集中的所有图像进行注释。注释意味着简单地在我们想要检测的每个对象周围绘制矩形框，并为其分配标签。我们将解释如何使用 Roboflow 进行此操作。</p><p><a href="https://roboflow.com/" target="_blank" rel="noopener noreferrer">Roboflow</a> 是一个基于在线的注释工具。在这里，我们可以直接将我们录制的视频片段导入 Roboflow，它将被导出为一系列图像。这个工具非常方便，因为它可以帮助我们将数据集分配到“训练、验证和测试”中。此外，此工具还允许我们在为图像添加标签后对这些图像进行进一步处理。此外，它可以轻松地将标记的数据集导出为我们所需要的 <strong>YOLOV5 PyTorch 格式</strong>！</p><p>对于本 Wiki，我们将使用一个包含苹果图像的数据集，以便我们可以稍后检测苹果并进行计数。</p><ul><li><strong>步骤 1.</strong> 单击<a href="https://app.roboflow.com/login" target="_blank" rel="noopener noreferrer">此处</a>注册 Roboflow 帐户</li><li><strong>步骤 2.</strong> 单击 <strong>创建新项目</strong> 开始我们的项目</li></ul></li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/YOLOV5/2.jpg" class="img_ev3q"></div><ul><li><strong>步骤 3.</strong> 填写 <strong>项目名称</strong>，保持 <strong>许可证（CC BY 4.0）</strong> 和 <strong>项目类型（目标检测（边界框））</strong> 默认。在 <strong>您的模型将预测什么？</strong> 列下，填写一个注释组名称。例如，在我们的情况下，我们选择 <strong>apples</strong>。这个名称应该突出显示您数据集中的所有类别。最后，点击 <strong>创建公共项目</strong>。</li></ul><div align="center"><img loading="lazy" width="350" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/6.jpg" class="img_ev3q"></div><ul><li><strong>步骤 4.</strong> 将使用 Grove - Vision AI 模块拍摄的图片拖放到此处。</li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/7.png" class="img_ev3q"></div><ul><li><strong>步骤 5.</strong> 图片处理完成后，点击 <strong>完成上传</strong>。耐心等待直到图片上传完成。</li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/4.jpg" class="img_ev3q"></div><ul><li><strong>Step 6.</strong> After the images are uploaded, click <strong>Assign Images</strong></li></ul><div align="center"><img loading="lazy" width="300" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/5.jpg" class="img_ev3q"></div><ul><li><strong>步骤 7.</strong> 选择一张图片，绘制一个围绕苹果的矩形框，选择标签为 <strong>apple</strong>，然后按下 <strong>回车键</strong>。</li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/9.png" class="img_ev3q"></div><ul><li><strong>步骤 8.</strong> 对其余的苹果重复相同的操作。</li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/10.png" class="img_ev3q"></div><p><strong>Note:</strong> <strong>尽量标注您在图像中看到的所有苹果。如果只有部分苹果可见，请尝试对其进行标注。</strong></p><ul><li><strong>步骤 9.</strong> 继续标注数据集中的所有图像。</li></ul><p>Roboflow 有一个称为“标签辅助”的功能，它可以预测标签，从而加快标注速度。但这不适用于所有类型的对象，而是一些特定类型的对象。要启用此功能，只需按下“标签辅助”按钮，选择一个模型，选择类别，然后浏览图像以查看带有边界框的预测标签。</p><div align="center"><img loading="lazy" width="300" src="https://files.seeedstudio.com/wiki/YOLOV5/41.png" class="img_ev3q"></div><div align="center"><img loading="lazy" width="400" src="https://files.seeedstudio.com/wiki/YOLOV5/39.png" class="img_ev3q"></div><div align="center"><img loading="lazy" width="400" src="https://files.seeedstudio.com/wiki/YOLOV5/40.png" class="img_ev3q"></div><ul><li>如上所示，它只能帮助预测上述80个类别的注释。如果您的图像不包含上述的对象类别，则无法使用标签辅助功能。<ul><li><strong>步骤 10：</strong> 标注完成后，单击 <strong>Add images to Dataset</strong></li></ul></li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/YOLOV5/25.jpg" class="img_ev3q"></div><ul><li><strong>步骤 11：</strong> 接下来，我们将图像分成 &quot;Train, Valid and Test&quot;。保持默认百分比的分布，然后点击 <strong>Add Images</strong>。</li></ul><div align="center"><img loading="lazy" width="330" src="https://files.seeedstudio.com/wiki/YOLOV5/26.png" class="img_ev3q"></div><ul><li><strong>步骤 12：</strong> 点击 <strong>生成新版本</strong></li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/YOLOV5/27.jpg" class="img_ev3q"></div><ul><li><strong>步骤 13：</strong> 现在您可以添加<strong>预处理</strong>和<strong>增强</strong>，如果您愿意的话。在这里，我们将<strong>调整</strong> <strong>调整大小</strong> 选项为 <strong>192x192</strong>。</li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/11.png" class="img_ev3q"></div><div align="center"><img loading="lazy" width="450" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/13.png" class="img_ev3q"></div><ul><li>在这里，我们将图像大小更改为192x192，因为我们将使用该大小进行训练，训练速度会更快。否则，在训练过程中，它将不得不将所有图像转换为192x192，这会消耗更多的CPU资源并使训练过程变慢。<ul><li><strong>步骤 14：</strong> 接下来，继续使用其余的默认设置，并单击 <strong>生成</strong>。</li></ul></li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/14.png" class="img_ev3q"></div><ul><li><strong>Step 15.</strong> Click <strong>Export</strong>, select <strong>Format</strong> as <strong>YOLO v5 PyTorch</strong>, select <strong>show download code</strong> and click <strong>Continue</strong></li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/54.png" class="img_ev3q"></div><p><strong>步骤 15：</strong> 点击 <strong>导出</strong>，选择 <strong>格式</strong> 为 <strong>YOLO v5 PyTorch</strong>，选择 <strong>显示下载代码</strong>，然后点击 <strong>继续</strong>。</p><div align="center"><img loading="lazy" width="600" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/55.png" class="img_ev3q"></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="在-google-colab-上使用-yolov5-进行训练-1">在 Google Colab 上使用 YOLOv5 进行训练<a href="#在-google-colab-上使用-yolov5-进行训练-1" class="hash-link" aria-label="Direct link to 在 Google Colab 上使用 YOLOv5 进行训练" title="Direct link to 在 Google Colab 上使用 YOLOv5 进行训练">​</a></h3><p>在完成了数据集的标注之后，我们需要对数据集进行训练。跳转到<a href="https://wiki.seeedstudio.com/Train-Deploy-AI-Model/#train-using-yolov5-on-google-colab" target="_blank" rel="noopener noreferrer">此部分</a>，该部分将解释如何在 Google Colab 上使用 YOLOv5 训练 AI 模型。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-deploy-the-trained-model-and-perform-inference"><span id="jump3">3. Deploy the trained model and perform inference</span><a href="#3-deploy-the-trained-model-and-perform-inference" class="hash-link" aria-label="Direct link to 3-deploy-the-trained-model-and-perform-inference" title="Direct link to 3-deploy-the-trained-model-and-perform-inference">​</a></h2><ul><li><h3 class="anchor anchorWithStickyNavbar_LWe7" id="部署训练模型并执行推断">部署训练模型并执行推断<a href="#部署训练模型并执行推断" class="hash-link" aria-label="Direct link to 部署训练模型并执行推断" title="Direct link to 部署训练模型并执行推断">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="grove---vision-ai-module">Grove - Vision AI Module<a href="#grove---vision-ai-module" class="hash-link" aria-label="Direct link to Grove - Vision AI Module" title="Direct link to Grove - Vision AI Module">​</a></h3><p>现在我们将把在训练结束时得到的 <strong>model-1.uf2</strong> 移动到 Grove - Vision AI 模块中。在这里，我们将通过将 Grove - Vision AI 模块与 <a href="https://www.seeedstudio.com/Wio-Terminal-p-4509.html" target="_blank" rel="noopener noreferrer">Wio Terminal</a> 连接，查看推断结果。</p><p><strong>注意：</strong> 如果这是您第一次使用 Arduino，请强烈建议您参考 <a href="https://wiki.seeedstudio.com/Getting_Started_with_Arduino" target="_blank" rel="noopener noreferrer">Getting Started with Arduino</a>。此外，请按照 <a href="https://wiki.seeedstudio.com/Wio-Terminal-Getting-Started/#getting-started" target="_blank" rel="noopener noreferrer">此 Wiki</a> 中的说明设置 Wio Terminal 以在 Arduino IDE 中使用。</p><ul><li><strong>步骤 1.</strong> 安装最新版本的 <a href="https://www.google.com/chrome" target="_blank" rel="noopener noreferrer">Google Chrome 浏览器</a> 或 <a href="https://www.microsoft.com/en-us/edge?r=1" target="_blank" rel="noopener noreferrer">Microsoft Edge 浏览器</a>，然后打开它。</li><li><strong>步骤 2.</strong> 通过 USB Type-C 线将 Grove - Vision AI 模块连接到您的计算机。</li></ul></li></ul><div align="center"><img loading="lazy" width="450" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/47.png" class="img_ev3q"></div><ul><li><strong>步骤 3.</strong> 双击 Grove - Vision AI 模块上的引导按钮，进入大容量存储模式</li></ul><div align="center"><img loading="lazy" width="220" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/48.png" class="img_ev3q"></div><p>在此之后，你将在文件浏览器中看到一个名为 <strong>GROVEAI</strong> 的新存储驱动器。</p><div align="center"><img loading="lazy" width="280" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/19.jpg" class="img_ev3q"></div><ul><li><strong>Step 4.</strong> 将 <strong>model-1.uf2</strong> 文件拖放到 <strong>GROVEAI</strong> 驱动器中。</li></ul><p>一旦 uf2 文件完成复制到驱动器中，驱动器将消失。这意味着 uf2 文件已成功上传到模块中。</p><p><strong>注意：</strong> 如果你有 4 个准备好的模型文件，你可以逐个拖放每个模型。拖放第一个模型，等待其完成复制，然后再次进入启动模式，拖放第二个模型，依此类推。</p><ul><li><strong>Step 5.</strong> 在 Grove - Vision AI 模块仍然通过 USB 连接到电脑的情况下，通过 Grove I2C 端口将其连接到 Wio Terminal，连接方式如下：</li></ul><div align="center"><img loading="lazy" width="250" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/49.png" class="img_ev3q"></div><ul><li><p><strong>Step 6.</strong> 将 <a href="https://github.com/Seeed-Studio/Seeed_Arduino_GroveAI" target="_blank" rel="noopener noreferrer">Seeed_Arduino_GroveAI 库</a>安装到 Arduino IDE 中，并打开 <strong>object_detection.ino</strong> 示例。</p></li><li><p><strong>Step 7.</strong> 如果你只加载了一个模型（索引为 1）到 Grove - Vision AI 模块中，它将加载该模型。然而，如果你加载了多个模型，你可以通过更改 <strong>MODEL<em>EXT_INDEX</em>[value]</strong> 来<a href="https://github.com/Seeed-Studio/Seeed_Arduino_GroveAI/blob/master/examples/object_detection/object_detection.ino#L12" target="_blank" rel="noopener noreferrer">指定要使用的模型</a>，其中 value 可以是数字 1、2、3 或 4。</p></li></ul><div class="language-cpp codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-cpp codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)">// for example:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">if</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">ai</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token function" style="color:rgb(80, 250, 123)">begin</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">ALGO_OBJECT_DETECTION</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> MODEL_EXT_INDEX_2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li>上面的代码将加载索引为 2 的模型。<ul><li><strong>Step 8.</strong> 由于我们要检测苹果，我们将在<a href="https://github.com/Seeed-Studio/Seeed_Arduino_GroveAI/blob/master/examples/object_detection/object_detection.ino#L55" target="_blank" rel="noopener noreferrer">这里</a>稍作更改：</li></ul></li></ul><div class="language-cpp codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-cpp codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Serial</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token function" style="color:rgb(80, 250, 123)">print</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Number of apples: &quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li><strong>Step 9.</strong> 将 Wio Terminal 连接到电脑，将此代码上传到 Wio Terminal，并在 Arduino IDE 中以 115200 作为波特率打开串行监视器。</li></ul><div align="center"><img loading="lazy" width="500" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/42.png" class="img_ev3q"></div><ul><li>你将能够在上述串行监视器上看到检测信息。<ul><li><strong>Step 10.</strong> <a href="https://files.seeedstudio.com/grove_ai_vision/index.html" target="_blank" rel="noopener noreferrer">点击这里</a> 打开一个具有检测功能的摄像头流的预览窗口。</li></ul></li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/31.png" class="img_ev3q"></div><ul><li><strong>步骤 11.</strong> 点击 <strong>连接</strong> 按钮。然后你会在浏览器上看到一个弹窗。选择 <strong>Grove AI - Paired</strong> 并点击 <strong>连接</strong>。</li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/32.png" class="img_ev3q"></div><ul><li><strong>Step 12.</strong> View real-time inference results using the preview window!</li></ul><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/33.jpg" class="img_ev3q"></div><p>As you can see above, the apples are being detected with bounding boxes around them. Here &quot;0&quot; corresponds to each detection of the same class. If you have multiple classes, they will be named as 0,1,2,3,4 and so on. Also the confidence score for each detected apple (0.8 and 0.84 in above demo) is being displayed!</p><ul><li><h3 class="anchor anchorWithStickyNavbar_LWe7" id="附加内容">附加内容<a href="#附加内容" class="hash-link" aria-label="Direct link to 附加内容" title="Direct link to 附加内容">​</a></h3><p>如果你感兴趣，你也可以继续阅读本维基的其余部分！</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="我可以在我的个人电脑上训练-ai-模型吗">我可以在我的个人电脑上训练 AI 模型吗？<a href="#我可以在我的个人电脑上训练-ai-模型吗" class="hash-link" aria-label="Direct link to 我可以在我的个人电脑上训练 AI 模型吗？" title="Direct link to 我可以在我的个人电脑上训练 AI 模型吗？">​</a></h3><p>你也可以使用自己的个人电脑来训练目标检测模型。但是，训练的性能取决于你的硬件条件。你还需要一台安装了 Linux 操作系统的个人电脑来进行训练。我们在这个维基中使用了一个 Ubuntu 20.04 个人电脑。</p><ul><li><strong>步骤 1.</strong> 在一个 <strong>Python&gt;=3.7.0</strong> 环境中克隆 <strong>yolov5-swift 仓库</strong> 并安装 <strong>requirements.txt</strong></li></ul></li></ul><div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">git clone https://github.com/Seeed-Studio/yolov5-swift </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd yolov5-swift</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">pip install -r requirements.txt</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li><strong>步骤 2.</strong> 如果你之前跟随本维基的步骤，你可能还记得我们在 Robolflow 中完成标注后导出数据集。此外，在 Roboflow Universe 中，我们下载了数据集。在这两种方法中，都会出现如下窗口询问要下载数据集的格式。因此现在，请选择 <strong>download zip to computer</strong>，在 <strong>Format</strong> 下选择 <strong>YOLO v5 PyTorch</strong> 并点击 <strong>Continue</strong>。</li></ul><div align="center"><img loading="lazy" width="400" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/16.png" class="img_ev3q"></div><div align="center"><img loading="lazy" width="400" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/17.png" class="img_ev3q"></div><ul><li>之后，一个 <strong>.zip 文件</strong> 将会下载到你的电脑上。<ul><li><strong>步骤 3.</strong> 将我们下载的 .zip 文件复制并粘贴到 <strong>yolov5-swift</strong> 目录中，然后解压缩。</li></ul></li></ul><div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># example</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cp ~/Downloads/Apples.v1i.yolov5pytorch.zip ~/yolov5-swift</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">unzip Apples.v1i.yolov5pytorch.zip</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li><strong>步骤 4.</strong> 打开 <strong>data.yaml</strong> 文件，将 <strong>train</strong> 和 <strong>val</strong> 目录编辑如下所示:</li></ul><div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">train: train/images</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">val: valid/images</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li><strong>步骤 5.</strong> 下载适合我们训练的预训练模型</li></ul><div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo apt install wget</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https://github.com/Seeed-Studio/yolov5-swift/releases/download/v0.1.0-alpha/yolov5n6-xiao.pt</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li><strong>步骤 6.</strong> 执行以下命令开始训练</li></ul><p>在这里，我们可以传递多个参数：</p><ul><li><strong>img:</strong> 定义输入图像大小</li><li><strong>batch:</strong> 确定批处理大小</li><li><strong>epochs:</strong> 定义训练周期数</li><li><strong>data:</strong> 设置到我们的 yaml 文件的路径</li><li><strong>cfg:</strong> 指定我们的模型配置</li><li><strong>weights:</strong> 指定自定义权重路径</li><li><strong>name:</strong> 结果名称</li><li><strong>nosave:</strong> 仅保存最终检查点</li><li><strong>cache:</strong> 为了更快的训练而缓存图像</li></ul><div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">python3 train.py --img 192 --batch 64 --epochs 100 --data data.yaml --cfg yolov5n6-xiao.yaml --weights yolov5n6-xiao.pt --name yolov5n6_results --cache</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>对于包含 987 张图像的苹果检测数据集，在本地 PC 上，使用 NVIDIA GeForce GTX 1660 Super GPU（6GB GPU 内存）完成训练过程大约需要 30 分钟。</p><div align="center"><img loading="lazy" width="1000" src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/44.png" class="img_ev3q"></div><ul><li>如果您按照上述 Colab 项目进行操作，您会了解到可以一次将 4 个模型加载到设备上。但是，请注意一次只能加载一个模型。用户可以指定加载的模型，这将在本 wiki 中进行解释。<ul><li><strong>步骤 7.</strong> 如果您导航到 <code>runs/train/exp/weights</code>，您将看到一个名为 <strong>best.pt</strong> 的文件。这是训练生成的模型。</li></ul></li></ul><div align="center"><img loading="lazy" width="600" src="https://files.seeedstudio.com/wiki/YOLOV5/33.jpg" class="img_ev3q"></div><ul><li><strong>步骤 8.</strong> 将训练好的模型导出为 TensorFlow Lite 格式</li></ul><div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">python3 export.py --data {dataset.location}/data.yaml --weights runs/train/yolov5n6_results/weights/best.pt --imgsz 192 --int8 --include tflite  </span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li><strong>步骤 9.</strong> 将 TensorFlow Lite 转换为 UF2 文件</li></ul><p>UF2 是由 Microsoft 开发的一种文件格式。Seeed 使用这种格式将 .tflite 文件转换为 .uf2 文件，使 tflite 文件可以存储在 Seeed 推出的 AIoT 设备上。目前，Seeed 的设备支持最多 4 个模型，每个模型（.tflite）小于 1M 。</p><p>您可以使用 -t 参数指定要放置在相应索引中的模型。</p><p>例如:</p><ul><li><code>-t 1</code>：索引 1</li><li><code>-t 2</code>：索引 2</li></ul><div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Place the model to index 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">python3 uf2conv.py -f GROVEAI -t 1 -c runs//train/yolov5n6_results//weights/best-int8.tflite -o model-1.uf2</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li>尽管您可以一次加载 4 个模型到设备上，但请注意一次只能加载一个模型。这可以由用户指定，并将在本 wiki 中进行解释。<ul><li><strong>步骤 10.</strong> 现在将生成一个名为 <strong>model-1.uf2</strong> 的文件。这是我们将加载到 Grove - Vision AI 模块中执行推理的文件！</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="资源">资源<a href="#资源" class="hash-link" aria-label="Direct link to 资源" title="Direct link to 资源">​</a></h2><ul><li><strong>[网页]</strong> <a href="https://docs.ultralytics.com/" target="_blank" rel="noopener noreferrer">YOLOv5 文档</a></li><li><strong>[网页]</strong> <a href="https://ultralytics.com/hub" target="_blank" rel="noopener noreferrer">Ultralytics HUB</a></li><li><strong>[网页]</strong> <a href="https://docs.roboflow.com/" target="_blank" rel="noopener noreferrer">Roboflow 文档</a></li><li><strong>[网页]</strong> <a href="https://www.tensorflow.org/lite/guide" target="_blank" rel="noopener noreferrer">TensorFlow Lite 文档</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="技术支持和产品讨论">技术支持和产品讨论<a href="#技术支持和产品讨论" class="hash-link" aria-label="Direct link to 技术支持和产品讨论" title="Direct link to 技术支持和产品讨论">​</a></h2><p>感谢您选择我们的产品！我们在这里为您提供不同的支持，以确保您对我们的产品的体验尽可能顺利。我们提供几种不同的沟通渠道，以满足不同的偏好和需求。s</p><div class="button_tech_support_container"><a href="https://forum.seeedstudio.com/" target="_blank" rel="noopener noreferrer" class="button_forum"></a><a href="https://www.seeedstudio.com/contacts" target="_blank" rel="noopener noreferrer" class="button_email"></a></div><div class="button_tech_support_container"><a href="https://discord.gg/eWkprNDMU7" target="_blank" rel="noopener noreferrer" class="button_discord"></a><a href="https://github.com/Seeed-Studio/wiki-documents/discussions/69" target="_blank" rel="noopener noreferrer" class="button_discussion"></a></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/zh-CN/Sensor/Grove/Grove_Sensors/AI-powered/cn-Train-Deploy-AI-Model-Grove-Vision-AI.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2023-03-16T00:00:00.000Z">Mar 16, 2023</time></b> by <b>DuKaiyin</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div><div>Loading Comments...</div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#可升级为工业传感器" class="table-of-contents__link toc-highlight">可升级为工业传感器</a></li><li><a href="#概述" class="table-of-contents__link toc-highlight">概述</a></li><li><a href="#硬件介绍" class="table-of-contents__link toc-highlight">硬件介绍</a><ul><li><a href="#grove---vision-ai模块" class="table-of-contents__link toc-highlight">Grove - Vision AI模块</a></li><li><a href="#什么是-roboflow" class="table-of-contents__link toc-highlight">什么是 Roboflow？</a></li><li><a href="#什么是-yolov5" class="table-of-contents__link toc-highlight">什么是 YOLOv5？</a></li><li><a href="#什么是-tensorflow-lite" class="table-of-contents__link toc-highlight">什么是 TensorFlow Lite？</a></li></ul></li><li><a href="#wiki-结构" class="table-of-contents__link toc-highlight">Wiki 结构</a></li><li><a href="#1-1-使用公共数据集训练自己的-ai-模型" class="table-of-contents__link toc-highlight"><span id="jump1">1. 1. 使用公共数据集训练自己的 AI 模型</span></a></li><li><a href="#部署和推理" class="table-of-contents__link toc-highlight">部署和推理</a></li><li><a href="#2-2-使用您自己的数据集训练自己的-ai-模型" class="table-of-contents__link toc-highlight"><span id="jump2">2. 2. 使用您自己的数据集训练自己的 AI 模型</span></a><ul><li><a href="#在-google-colab-上使用-yolov5-进行训练-1" class="table-of-contents__link toc-highlight">在 Google Colab 上使用 YOLOv5 进行训练</a></li></ul></li><li><a href="#3-deploy-the-trained-model-and-perform-inference" class="table-of-contents__link toc-highlight"><span id="jump3">3. Deploy the trained model and perform inference</span></a></li><li><a href="#资源" class="table-of-contents__link toc-highlight">资源</a></li><li><a href="#技术支持和产品讨论" class="table-of-contents__link toc-highlight">技术支持和产品讨论</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Navigation</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Getting_Started/">Getting Started</a></li><li class="footer__item"><a class="footer__link-item" href="/Sensor_Network/">Sensor Network</a></li><li class="footer__item"><a class="footer__link-item" href="/Edge_Computing/">Edge Computing</a></li><li class="footer__item"><a class="footer__link-item" href="/CloudnChain/">Cloud &amp; Chain</a></li><li class="footer__item"><a class="footer__link-item" href="/Solutions/">Solutions</a></li></ul></div><div class="col footer__col"><div class="footer__title">Ecosystem</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discord.com/invite/QqMgVwHT3X" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord</a></li><li class="footer__item"><a href="https://project.seeedstudio.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Project Hub</a></li><li class="footer__item"><a href="https://www.seeedstudio.com/ecosystem/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Partners</a></li><li class="footer__item"><a href="https://www.seeedstudio.com/distributors.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Distributors</a></li></ul></div><div class="col footer__col"><div class="footer__title">Quick Guide</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.seeedstudio.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Bazzar</a></li><li class="footer__item"><a href="https://www.seeedstudio.com/get_help/HowToGetHelp" target="_blank" rel="noopener noreferrer" class="footer__link-item">How to get help</a></li><li class="footer__item"><a href="https://support.seeedstudio.com/knowledgebase" target="_blank" rel="noopener noreferrer" class="footer__link-item">FAQs</a></li><li class="footer__item"><a href="https://forum.seeedstudio.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Forum</a></li><li class="footer__item"><a href="https://www.seeedstudio.com/get_help/TechnicalSupport" target="_blank" rel="noopener noreferrer" class="footer__link-item">Technical Support</a></li></ul></div><div class="col footer__col"><div class="footer__title">Company</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.seeedstudio.com/about-us/" target="_blank" rel="noopener noreferrer" class="footer__link-item">About Seeed</a></li><li class="footer__item"><a href="https://www.seeedstudio.com/join-us/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Join us</a></li><li class="footer__item"><a href="https://www.seeedstudio.com/contacts" target="_blank" rel="noopener noreferrer" class="footer__link-item">Contact Us</a></li><li class="footer__item"><a href="https://www.seeedstudio.com/blog/2020/04/22/seeed-in-the-news/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Press</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Seeed Studio, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.fe3ab845.js"></script>
<script src="/assets/js/main.7eadbfa1.js"></script>
</body>
</html>