"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1076],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>h});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),m=p(n),c=i,h=m["".concat(l,".").concat(c)]||m[c]||d[c]||o;return n?a.createElement(h,r(r({ref:t},u),{},{components:n})):a.createElement(h,r({ref:t},u))}));function h(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,r=new Array(o);r[0]=c;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[m]="string"==typeof e?e:i,r[1]=s;for(var p=2;p<o;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},87632:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var a=n(87462),i=(n(67294),n(3905));const o={description:"How to Run a Local LLM Text-to-Image on reComputer",title:"How to Run a Local LLM Text-to-Image on reComputer",keywords:["Contributor"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/How_to_run_local_llm_text_to_image_on_reComputer",last_update:{date:"04/01/2024",author:"Bruno"}},r="How to Run a Local LLM Text-to-Image on reComputer",s={unversionedId:"Edge/NVIDIA_Jetson/Application/Generative_AI/How_to_run_local_llm_text_to_image_on_reComputer",id:"Edge/NVIDIA_Jetson/Application/Generative_AI/How_to_run_local_llm_text_to_image_on_reComputer",title:"How to Run a Local LLM Text-to-Image on reComputer",description:"How to Run a Local LLM Text-to-Image on reComputer",source:"@site/docs/Edge/NVIDIA_Jetson/Application/Generative_AI/How_to_run_local_llm_text_to_image_on_reComputer.md",sourceDirName:"Edge/NVIDIA_Jetson/Application/Generative_AI",slug:"/How_to_run_local_llm_text_to_image_on_reComputer",permalink:"/How_to_run_local_llm_text_to_image_on_reComputer",draft:!1,editUrl:"https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/Edge/NVIDIA_Jetson/Application/Generative_AI/How_to_run_local_llm_text_to_image_on_reComputer.md",tags:[],version:"current",lastUpdatedBy:"Bruno",lastUpdatedAt:1711929600,formattedLastUpdatedAt:"Apr 1, 2024",frontMatter:{description:"How to Run a Local LLM Text-to-Image on reComputer",title:"How to Run a Local LLM Text-to-Image on reComputer",keywords:["Contributor"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/How_to_run_local_llm_text_to_image_on_reComputer",last_update:{date:"04/01/2024",author:"Bruno"}},sidebar:"ProductSidebar",previous:{title:"Whisper on Jetson for Real time Speech to Text",permalink:"/Edge/NVIDIA_Jetson/Application/Generative_AI/Whisper_on_Jetson_for_Real_Time_Speech_to_Text"},next:{title:"Getting Started with Allxon",permalink:"/Allxon-Jetson-Getting-Started"}},l={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Overview",id:"overview",level:2},{value:"Troubleshooting",id:"troubleshooting",level:3},{value:"Requirements",id:"requirements",level:2},{value:"Step 1 - Create the virtual environments",id:"step-1---create-the-virtual-environments",level:3},{value:"TensorFlow",id:"tensorflow",level:5},{value:"PyTorch",id:"pytorch",level:5},{value:"1.1 Keras",id:"11-keras",level:4},{value:"Step 1.2 - Hugging Face",id:"step-12---hugging-face",level:3},{value:"Virtual environment",id:"virtual-environment",level:4},{value:"Model",id:"model",level:4},{value:"Step 1.3 - Create a small API",id:"step-13---create-a-small-api",level:3},{value:"Step 2 - Nvidia LLM",id:"step-2---nvidia-llm",level:3},{value:"Stable Diffusion v1.5",id:"stable-diffusion-v15",level:4},{value:"Stable Diffusion XL",id:"stable-diffusion-xl",level:4},{value:"Adding other models",id:"adding-other-models",level:4},{value:"\u2728 Contributor Project",id:"-contributor-project",level:2},{value:"Tech Support &amp; Product Discussion",id:"tech-support--product-discussion",level:2}],u={toc:p},m="wrapper";function d(e){let{components:t,...n}=e;return(0,i.kt)(m,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"how-to-run-a-local-llm-text-to-image-on-recomputer"},"How to Run a Local LLM Text-to-Image on reComputer"),(0,i.kt)("h2",{id:"introduction"},"Introduction"),(0,i.kt)("p",null,"A Text-to-image model is a type of artificial intelligence (AI) model that generates images from a textual description. These models take textual input, like sentences or paragraphs describing a scene and produce an image based on that description. "),(0,i.kt)("p",null,"These models are trained on large datasets containing pairs of text descriptions and corresponding images, learning to understand the relationships between textual and visual information. "),(0,i.kt)("p",null,"Text-to-image models have made significant progress in recent years, but generating high-quality and diverse images that accurately match textual descriptions remains a challenging task in AI research."),(0,i.kt)("h2",{id:"overview"},"Overview"),(0,i.kt)("p",null,"On this tutorial, we're going to explore several ways to deploy and run a local LLM text-to-image:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Create the virtual environment (both TensorFlow and PyTorch)",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"1.1. Create an example with Keras Stable Diffusion"),(0,i.kt)("li",{parentName:"ul"},"1.2. Create an example using one of the models available at Hugging Face"),(0,i.kt)("li",{parentName:"ul"},"1.3. Create a small Python API that we will use to generate images by calling the API for both Keras and Hugging Face"))),(0,i.kt)("li",{parentName:"ol"},"Using a Nvidia container.")),(0,i.kt)("h3",{id:"troubleshooting"},"Troubleshooting"),(0,i.kt)("p",null,"Before we start, here's some steps that we can take in order to have more memory available to us."),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Disable the Desktop GUI. We can use the Jetson though SSH. We can save around ~800MB of memory.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Disable ZRAM and use Swap. "))),(0,i.kt)("p",null,"You can find those tips in the ",(0,i.kt)("a",{parentName:"p",href:"https://www.jetson-ai-lab.com/tips_ram-optimization.html"},"Nvidia Jetson AI Lab")," and how to implement them. "),(0,i.kt)("h2",{id:"requirements"},"Requirements"),(0,i.kt)("p",null,"For this tutorial, we're going to need a Nvidia Jetson Orin NX 16GB."),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_J4012.png"})),(0,i.kt)("div",{class:"get_one_now_container",style:{textAlign:"center"}},(0,i.kt)("a",{class:"get_one_now_item",href:"https://www.seeedstudio.com/reComputer-J4012-p-5586.html?queryID=3d7dba9378be2accafeaff54420edb6a&objectID=5586&indexName=bazaar_retailer_products"},(0,i.kt)("strong",null,(0,i.kt)("span",null,(0,i.kt)("font",{color:"FFFFFF",size:"4"}," Get One Now \ud83d\uddb1\ufe0f"))))),(0,i.kt)("p",null,"And we're going to need to make sure that TensorFlow and PyTorch are installed - but I'm going through that here. "),(0,i.kt)("h3",{id:"step-1---create-the-virtual-environments"},"Step 1 - Create the virtual environments"),(0,i.kt)("p",null,"Keras can use TensorFlow or PyTorch as backends. Hugging Face will mostly use PyTorch"),(0,i.kt)("p",null,"Let's install TensorFlow and PyTorch. "),(0,i.kt)("p",null,"The instructions on how to install TensorFlow and PyTorch for the Jetson Orin NX are in the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html"},"Nvidia Website"),"."),(0,i.kt)("p",null,"We can install TensorFlow and PyTorch globally or on a virtual Environment. We're be using a virtual environment. "),(0,i.kt)("p",null,"By using a Virtual Environment we don't run the risk of mixing projects or packages versions. "),(0,i.kt)("p",null,"This is the best way, although the Nvidia site prefers the global method. "),(0,i.kt)("h5",{id:"tensorflow"},"TensorFlow"),(0,i.kt)("p",null,"Create the virtual Environment (I'm using the name kerasStableEnvironment because I'm going to use it for the keras example. Use other name if you want.)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"sudo apt install python3.8-venv\npython -m venv kerasStableEnvironment\n")),(0,i.kt)("p",null,"After creating it, activate the virtual environment"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"source kerasStableEnvironment/bin/activate\n")),(0,i.kt)("p",null,"When active, you'll see the name of it before the prompt"),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/1_prompt_bash.png"})),(0,i.kt)("p",null,"Enter the virtual enviroment"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"cd kerasStableEnvironment\n")),(0,i.kt)("p",null,"Upgrade PIP and install some dependencies"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install -U pip\npip install -U numpy grpcio absl-py py-cpuinfo psutil portpicker six mock requests gast h5py astor termcolor protobuf keras-applications keras-preprocessing wrapt google-pasta setuptools testresources\n")),(0,i.kt)("p",null,"Install TensorFlow for Jetpack 5.1.1"),(0,i.kt)("p",null,"To find what JetPack version we have, issue the following command:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"dpkg -l | grep -i jetpack\n")),(0,i.kt)("p",null,"and the result should show the jetpack version:"),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/2_jetpack_version.png"})),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v511 tensorflow==2.12.0+nv23.05\n")),(0,i.kt)("p",null,"If you have another JetPack version, check ",(0,i.kt)("a",{parentName:"p",href:"https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html"},"Nvidia Website")," for the correct URL. "),(0,i.kt)("p",null,"Now, let's check TensorFlow installation "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n")),(0,i.kt)("p",null,"This should return the following line:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n")),(0,i.kt)("h5",{id:"pytorch"},"PyTorch"),(0,i.kt)("p",null,"Let's install some dependencies"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"sudo apt install libopenblas-dev\n")),(0,i.kt)("p",null,"Now, install PyTorch for JetPack 5.1.1"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install --no-cache https://developer.download.nvidia.com/compute/redist/jp/v511/pytorch/torch-2.0.0+nv23.05-cp38-cp38-linux_aarch64.whl\n")),(0,i.kt)("p",null,"To check the installation and if CUDA is available"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'python -c "import torch; print(torch.cuda.is_available())"\n')),(0,i.kt)("p",null,"It should return ",(0,i.kt)("strong",{parentName:"p"},"True")),(0,i.kt)("p",null,"Now that we have both TensorFlow and PyTorch installed, let's install Keras and create an image"),(0,i.kt)("h4",{id:"11-keras"},"1.1 Keras"),(0,i.kt)("p",null,"After installing ",(0,i.kt)("strong",{parentName:"p"},"PyTorch")," and ",(0,i.kt)("strong",{parentName:"p"},"Tensorflow"),", we're now ready to start creating images from text prompts.\nBe sure that you're still on the virtual environment. "),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://keras.io/keras_cv/"},"KerasCV")," has an implementation (along several others) of ",(0,i.kt)("a",{parentName:"p",href:"https://stability.ai/"},"Stability.ai")," texto-to-image model, ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/CompVis/stable-diffusion"},"Stable Diffusion"),". "),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/"},"By using the KerasCV implementation"),", we can use some the performance advantages, like XLA compilation and mixed precision support. "),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/"},"You can read more on Keras Website")),(0,i.kt)("p",null,"Install keras and dependencies. We're going for this versions because they work with the TensorFlow (or PyTorch) versions that we have installed. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install keras-cv==0.5.1\npip install keras==2.12.0\npip install Pillow\n")),(0,i.kt)("p",null,"Open your preferred editor and type the following example"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"vi generate_image.py\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import keras_cv\nimport keras\nfrom PIL import Image\n\nkeras.mixed_precision.set_global_policy("mixed_float16")\n\nmodel = keras_cv.models.StableDiffusion (\n        img_width=512,  # we can choose another size, but has to be a mutiple of 128\n        img_height=512, # the same above\n        jit_compile=True\n)\n\nprompt = "a cute magical flying dog, fantasy art, golden color, high quality, highly detailed, elegant, sharp focus, concept art, character concepts, digital painting, mystery, adventure"\n\nimage = model.text_to_image (prompt,\n        num_steps = 25, #image quality\n        batch_size = 1 # how many images to generate at once\n)\n\nImage.fromarray(image[0]).save("keras_generate_image.png")\n')),(0,i.kt)("p",null,"While running the script, here are some statistics"),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/3_statistics.png"})),(0,i.kt)("p",null,"And after a while, here's the result"),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/4_keras_generate_image.png"})),(0,i.kt)("h3",{id:"step-12---hugging-face"},"Step 1.2 - Hugging Face"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://huggingface.co/"},"Hugging Face")," is like the Github for Machine Learning. It lets developers built, deploy, share and train their ML models."),(0,i.kt)("p",null,"Hugging Face is also known for their Transformers Python library, which eases the process of downloading and training ML models. "),(0,i.kt)("p",null,"Let's use some of the models available.\nHead to Hugging Face and choose to see the models."),(0,i.kt)("p",null,"On the left side, you have filters that allow us to choose what type of models we want to see."),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/5_huggingface.png"})),"There are a lot of models available, but we're going to concentrate on the text-to-image models.",(0,i.kt)("h4",{id:"virtual-environment"},"Virtual environment"),(0,i.kt)("p",null,"Create a Virtual Environment, like we did above, so we can use Hugging Face without messing packages versions or installing packages we don't need. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python -m venv huggingfaceTesting\nsource huggingfaceTesting/bin/activate\n")),(0,i.kt)("p",null,"After creating the virtual environment, let's enter it.\nInstall PyTorch using the instructions above. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"cd huggingfaceTesting\n")),(0,i.kt)("h4",{id:"model"},"Model"),(0,i.kt)("p",null,"Hugging Face has a lot of ",(0,i.kt)("a",{parentName:"p",href:"https://huggingface.co/models?pipeline_tag=text-to-image&sort=trending"},"text-to-image models"),". Although theoretically they should work with our Jetson, they don't. "),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"stable-diffusion-v1-5")),(0,i.kt)("p",null,"I'm going to test the ",(0,i.kt)("a",{parentName:"p",href:"https://huggingface.co/runwayml/stable-diffusion-v1-5"},"stable-diffusion-v1-5 from Runaway"),". "),(0,i.kt)("p",null,"On the model card, they have all the information necessary to work with the model . "),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/6_stable_diffusion_v1_5.png"})),(0,i.kt)("p",null,"We're going to use Hugging Face diffusers library.\nInside the virtual environment (and with it activated) install the dependencies. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install diffusers transformers accelerate\n")),(0,i.kt)("p",null,"Now that we have all the dependencies installed, let's try the model.\nUsing your favorite editor, copy the following code (also available in the model card page):"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = "runwayml/stable-diffusion-v1-5"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\n\nprompt = "a master jedi cat in star wars holding a lightsaber, wearing a jedi cloak hood, dramatic, cinematic lighting"\nimage = pipe(prompt).images[0]  \n    \nimage.save("cat_jedi.png")\n\n')),(0,i.kt)("p",null,"Let's try the model. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python stableDiffusion.py\n")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Remember:")," This takes a lot of space. The model's checkpoints are being downloaded. This will be done just one time. "),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/7_model_download.png"})),"After a while, here's the result",(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/8_result_stablediffusion.png"})),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"SDXL-Turbo")),(0,i.kt)("p",null,"Here's another model we can try. ",(0,i.kt)("a",{parentName:"p",href:"https://huggingface.co/stabilityai/sdxl-turbo"},"SDXL Turbo from Stability AI."),"\nCopy the following code"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from diffusers import AutoPipelineForText2Image\nimport torch\n\npipe = AutoPipelineForText2Image.from_pretrained("stabilityai/sdxl-turbo", torch_dtype=torch.float16, variant="fp16")\npipe.to("cuda")\n\nprompt = "full body, cat dressed as a Viking, with weapon in his paws, battle coloring, glow hyper-detail, hyper-realism, cinematic"\n\nimage = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\nimage.save("sdxl-turbo.png")\n')),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://medium.com/phygital/top-40-useful-prompts-for-stable-diffusion-xl-008c03dd0557"},"This prompt was taken from a Medium article written by Daria Wind")),(0,i.kt)("p",null,"This one is really fast generating an image. Takes almost 30s, from running the script until it exits.\nHere's the result"),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/9_sdxl-turbo.png"})),(0,i.kt)("p",null,"We can also try other models, like models trained specifically for anime or cyberpunk. "),(0,i.kt)("p",null,"There will be some models that will not work. It can be because of several factors - memory, available CPUs or even Swap memory. "),(0,i.kt)("h3",{id:"step-13---create-a-small-api"},"Step 1.3 - Create a small API"),(0,i.kt)("p",null,"Let's now crete a small API with Flask to use to generate an image given a prompt and return it to the caller. "),(0,i.kt)("p",null,"Imagine that you have the Jetson running and want to be able to generate an image by calling a API - your personal LLM image-to-text."),(0,i.kt)("p",null,"There are already projects that do this (like the one we're going to see later), but nothing beats doing it yourself. "),(0,i.kt)("p",null,"Let's create a new Virtual Environment"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python -m venv imageAPIGenerator\n")),(0,i.kt)("p",null,"Activate the environment and enter it"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"source  imageAPIGenerator/bin/activate\ncd imageAPIGenerator\n")),(0,i.kt)("p",null,"We're going go use Flask for this. ",(0,i.kt)("a",{parentName:"p",href:"https://flask.palletsprojects.com/en/3.0.x/"},"FlasK")," is web application framework written in Python. It's small enough for our purpose. "),(0,i.kt)("p",null,"Install Flask."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install Flask\n")),(0,i.kt)("p",null,"After installing it, let's install all the other dependencies that we need. Just for demonstration purposes, we're going to use Keras, because it has the least dependencies. "),(0,i.kt)("p",null,"Install TensorFlow. Follow the instructions above.\nNext, install Keras."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install keras-cv==0.5.1\npip install keras==2.12.0\npip install Pillow\n")),(0,i.kt)("p",null,"Now, let's start to write our application. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"vi app.py\n")),(0,i.kt)("p",null,"For those who don't know what Flask is or does, let's try a small example. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from flask import Flask\n\napp = Flask (__name__)\n\n\n@app.route("/generate_image")\ndef generate_image_api():\n    return "<h2>Hello World !</h2>"\n\n\nif __name__ == "__main__":\n    app.run(host=\'\',port=8080)\n')),(0,i.kt)("p",null,"To run, execute the python script:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python app.py\n")),(0,i.kt)("p",null,"You should see the following:"),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/10_run_flask.png"})),(0,i.kt)("p",null,"Now, open a browser and try to access your Jetson device with the 8080 port."),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/11_browser_access.png"}),(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/12_accessed_flask.png"})),(0,i.kt)("p",null,"What we did was importing the Flask class"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import Flask\n")),(0,i.kt)("p",null,"We next created an instance of the Flask class"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"app = Flask(__name__)\n")),(0,i.kt)("p",null,"We next create a route decorator to tell Flask what URL will trigger our function"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'@app.route("/generate_image")\n')),(0,i.kt)("p",null,"When using generate_image in the URL, we will trigger our function"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def generate_image_api():\n    return "<h2>Hello World !</h2>"\n')),(0,i.kt)("p",null,"We can also use curl to access our API"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"curl http://192.168.2.230:8080/generate_image\n")),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/13_curl.png"})),"Now that we know how to create a API, let's dive into it and write it.",(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"vi app.py\n")),(0,i.kt)("p",null,"And paste the code"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from flask import Flask, request, send_file\nimport random, string\nimport keras_cv\nimport keras\nfrom PIL import Image\n\n#define APP\napp = Flask (__name__)\n\n\n#option for keras\nkeras.mixed_precision.set_global_policy("mixed_float16")\n\n# generate custom filename\ndef generate_random_string(size):\n    """Generate a random string of specified size."""\n    return \'\'.join(random.choices(string.ascii_letters + string.digits, k=size))\n\n\n"""\n    This is the function that will generate the image\n    and save it using a random created filename\n"""\ndef generate_image(prompt):\n\n    model = keras_cv.models.StableDiffusion (\n        img_width=512,  # we can choose another size, but has to be a mutiple of 128\n        img_height=512, # the same above\n        jit_compile=True\n    )\n\n    image = model.text_to_image (prompt,\n            num_steps = 25,\n            batch_size = 1\n    )\n\n    # image filename\n    filename = generate_random_string(10) + ".png"\n    Image.fromarray(image[0]).save(filename)\n    return filename # return filename to send it to client\n\n\n#define routes\n# Use this to get the prompt. we\'re going to receive it using GET\n@app.route("/generate_image", methods=["GET"])\ndef generate_image_api():\n    # get the prompt\n    prompt = request.args.get("prompt")\n    if not prompt:\n        # let\'s define a default prompt\n        prompt = "A cinematic shot of a baby racoon wearing an intricate italian priest robe."\n\n    image_name = generate_image(prompt)\n    return send_file(image_name, mimetype=\'image/png\')\n\n\nif __name__ == "__main__":\n    app.run(host=\'0.0.0.0\',port=8080)\n')),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"REMEMBER:")," This is not code ready for the Internet. We don't have any security measures whatsoever. "),(0,i.kt)("p",null,"Let's run it."),(0,i.kt)("p",null,"In a browser, type in the URL ",(0,i.kt)("em",{parentName:"p"},"http://jetsonIP:8080/generate_image")," and wait. "),(0,i.kt)("p",null,"If we don't give it a prompt, it will use the default one we've set."),(0,i.kt)("p",null,"In the CLI, you can see the image being generated"),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/14_generating_image_api.png"})),(0,i.kt)("p",null,"And in the browser, after a while, we can see the image"),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/15_image_API_generated.png"})),(0,i.kt)("p",null,"We can also see the image has been sent"),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/16_cli_generated.png"})),(0,i.kt)("p",null,"We can also use curl to get the image and save it."),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/17_cli_generating.png"})),(0,i.kt)("p",null,"If we want to give it a prompt (as we should), the URL will look like\n",(0,i.kt)("em",{parentName:"p"},"http://jetsonIP:8080/generate_image?prompt=<your_prompt>")),(0,i.kt)("p",null,"We can expand this example to build a better page, like having some text boxes for user input, a nice background, etc. But this is for another project. "),(0,i.kt)("h3",{id:"step-2---nvidia-llm"},"Step 2 - Nvidia LLM"),(0,i.kt)("h4",{id:"stable-diffusion-v15"},"Stable Diffusion v1.5"),(0,i.kt)("p",null,"We can use the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/dusty-nv/jetson-containers"},"Jetson Containers")," project to run ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},"stable-diffusion-webui using AUTOMATIC1111"),".\nJetson Containers project is run by ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/dusty-nv"},"Dusty Franklin"),", a NVIDIA employee."),(0,i.kt)("p",null,"NVIDIA has the ",(0,i.kt)("a",{parentName:"p",href:"https://www.jetson-ai-lab.com/tutorial-intro.html"},"NVIDIA Jetson Generative AI Lab")," project that has a lot of tutorials on Machine Learning. "),(0,i.kt)("p",null,"We're going to use ",(0,i.kt)("a",{parentName:"p",href:"https://www.jetson-ai-lab.com/tutorial_stable-diffusion.html"},"Stable Diffusion tutorial")," for this. "),(0,i.kt)("p",null,"Let's clone the github repository, enter the repository and install dependencies"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers/\nsudo apt update; sudo apt install -y python3-pip\npip3 install -r requirements.txt\n")),(0,i.kt)("p",null,"Now that we have everything we need, let's run the container with the ",(0,i.kt)("em",{parentName:"p"},"stable-diffusion-webui autotag")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"./run.sh $(./autotag stable-diffusion-webui)\n")),(0,i.kt)("p",null,"It will start to run the container. "),(0,i.kt)("p",null,"After a while, it will say that there's a compatible container and if we want to proceed. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"Found compatible container dustynv/stable-diffusion-webui:r35.3.1 (2024-02-02, 7.3GB) - would you like to pull it? [Y/n] \n")),(0,i.kt)("p",null,"It will start downloading the container. "),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/18_container_downloading.png"})),(0,i.kt)("p",null,"After finishing, it will download the model and run the server on port 7860."),(0,i.kt)("p",null,"Here for me it didn't work at first. No checkpoint would appear to choose from, no matter how many times I would press the refresh\nbutton. "),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/20_no_checkpoint.png"})),(0,i.kt)("p",null,"I found out that I had 100% space occupied. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"feiticeir0@JetsonOrin:~$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme0n1p1   79G   79G     0 100% /\nnone            7,4G     0  7,4G   0% /dev\ntmpfs           7,6G     0  7,6G   0% /dev/shm\ntmpfs           1,6G   19M  1,5G   2% /run\ntmpfs           5,0M  4,0K  5,0M   1% /run/lock\ntmpfs           7,6G     0  7,6G   0% /sys/fs/cgroup\n/dev/loop0      162M  162M     0 100% /snap/chromium/2797\n/dev/loop2      128K  128K     0 100% /snap/bare/5\n/dev/loop1       70M   70M     0 100% /snap/core22/1125\n/dev/loop3       65M   65M     0 100% /snap/cups/1025\n/dev/loop4       92M   92M     0 100% /snap/gtk-common-themes/1535\n/dev/loop6      162M  162M     0 100% /snap/chromium/2807\n/dev/loop5      483M  483M     0 100% /snap/gnome-42-2204/174\n/dev/loop7       35M   35M     0 100% /snap/snapd/21185\ntmpfs           1,6G  4,0K  1,6G   1% /run/user/1000\n")),(0,i.kt)("p",null,"I've been testing other models and they occupied all the space.\nIf this happens to you, just go to your home directory, to the hidden cache directory and delete the huggingface directory. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"cd ~/.cache\nrm -rf huggingface\n")),(0,i.kt)("p",null,"Now you should have space available. Or just get a new drive, with more space. :)"),(0,i.kt)("p",null,"Now the model is being downloaded."),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/21_mode_downloading.png"})),"And we have a checkpoint",(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/22_checkpoint.png"})),(0,i.kt)("p",null,"Open your browser and head to your Jetson IP address and port to run the AUTOMATIC1111's Stable Diffusion webgui"),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"http://JetsonIPAddress:7860")),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/19_jetson_webgui.png"})),(0,i.kt)("p",null,"Now we can play with it.\nHere's some images created with the default model. "),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/23_creating_image1.gif"})),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/24_creating_image2.gif"})),(0,i.kt)("h4",{id:"stable-diffusion-xl"},"Stable Diffusion XL"),(0,i.kt)("p",null,"AUTOMATIC1111 supports other models. Let's try with Stable Diffusion XL. It has 6.6 billion parameters. "),(0,i.kt)("p",null,"To add another model, and to be easier to download it, let's define some variables, change permissions and download the models.\nThis is an example from ",(0,i.kt)("a",{parentName:"p",href:"https://www.jetson-ai-lab.com/tutorial_stable-diffusion-xl.html"},"NVIDIA's Tutorial"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"CONTAINERS_DIR=<where_jetson-containers_is_located>\nMODEL_DIR=$CONTAINERS_DIR/data/models/stable-diffusion/models/Stable-diffusion/\nsudo chown -R $USER $MODEL_DIR\n")),(0,i.kt)("p",null,"Now, download the model"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"wget -P $MODEL_DIR https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\nwget -P $MODEL_DIR https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0.safetensors\n")),(0,i.kt)("p",null,"With the models downloaded, let's refresh the checkpoints drop-down if you have the container running, or launch the container again. "),(0,i.kt)("p",null,"Now we have two more models available to us. "),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/25_models.png"})),(0,i.kt)("p",null,"This is an example generated with the XL model, with the following prompt:"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"A Portrait, fashionable model wearing futuristic clothing, in a cyberpunk roof-top environment, with a neon-lit city background, backlit by vibrant city glow, fashion photography")),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/26_neon_xl.png"})),"Try it. Remember that, it may not work with some options selected.",(0,i.kt)("h4",{id:"adding-other-models"},"Adding other models"),(0,i.kt)("p",null,"We can also add a lot of more models. Besides Hugging Face, ",(0,i.kt)("a",{parentName:"p",href:"https://civitai.com/"},"Civitai")," is another hub with more models to choose from. Civitai has some NSFW models, so consider yourself warned. "),(0,i.kt)("p",null,"Select the one you want, download the checkpoints and place them in the models directory"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"/home/<user>/<jetson-containers-location>/data/models/stable-diffusion/models/Stable-diffusion/\n")),(0,i.kt)("p",null,"I'm going to download and try a model named ",(0,i.kt)("a",{parentName:"p",href:"https://civitai.com/models/112902/dreamshaper-xl"},"DreamShaper XL"),". "),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/27_dreamshaperxl.png"})),(0,i.kt)("p",null,"Remember that some models may not work. "),(0,i.kt)("p",null,"You need to play with the settings and read the model card to know what settings may work best (if at all). "),(0,i.kt)("p",null,"For example, this model card says that the sampling steps should be 4-8, sampling method should be DPM++ SDE Karras, etc... "),(0,i.kt)("p",null,"Download the model checkpoint and add it to the above directory. "),(0,i.kt)("p",null,"After refreshing , you should have the model ready to select.\nWhen selecting, AUTOMATIC1111 will optimize the model. "),(0,i.kt)("p",null,"If it gets getting killed or an error appears, get more space. It was happening to me and after getting more space, everything worked out. "),(0,i.kt)("p",null,"Using the following prompt"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"holding a staff, orbstaff <lora:orbstaff:0.60> , ,(by Gabriel Isak and Adam Elsheimer:1.20), (by Jon Whitcomb and Bayard Wu and Malcolm Liepke0.80),8k , professional fashion shot")),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://civitai.com/images/8570722"},"from this image"),",\nwithout the negative prompt, I got the following result"),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/28_dreamshaperxl_image_result.png"})),(0,i.kt)("p",null,"with these settings:"),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/29_dreamshaperXL_settings.png"})),(0,i.kt)("p",null,"Remeber the prompt above for the cyberpunk girl using the ",(0,i.kt)("em",{parentName:"p"},"Stable Diffusion XL")," model? "),(0,i.kt)("p",null,"Here's a new image, with the same prompt, generated with ",(0,i.kt)("em",{parentName:"p"},"DreamShaper XL")," with the same settings above"),(0,i.kt)("div",{align:"center"},(0,i.kt)("img",{width:800,src:"https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/30_cyberpunkGirl.png"})),(0,i.kt)("p",null,"As you can see, wonderful images can be created, granting that you know the parameters to tune. :)"),(0,i.kt)("p",null,"I've learned that bigger images tend to produce better results. "),(0,i.kt)("p",null,"Hope you've learn how to generate images using the Nvidia Jetson NX 16GB and how to use it as a server to generate images on demand. "),(0,i.kt)("h2",{id:"-contributor-project"},"\u2728 Contributor Project"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"This project is supported by the Seeed Studio ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/orgs/Seeed-Studio/projects/6/views/1?pane=issue&itemId=56418890"},"Contributor Project"),"."),(0,i.kt)("li",{parentName:"ul"},"Thanks ",(0,i.kt)("a",{parentName:"li",href:"https://github.com/Seeed-Studio/wiki-documents/issues/1029"},"Bruno's efforts")," and your work will be ",(0,i.kt)("a",{parentName:"li",href:"https://wiki.seeedstudio.com/Honorary-Contributors/"},"exhibited"),".")),(0,i.kt)("h2",{id:"tech-support--product-discussion"},"Tech Support & Product Discussion"),(0,i.kt)("p",null,"Thank you for choosing our products! We are here to provide you with different support to ensure that your experience with our products is as smooth as possible. We offer several communication channels to cater to different preferences and needs."),(0,i.kt)("div",{class:"button_tech_support_container"},(0,i.kt)("a",{href:"https://forum.seeedstudio.com/",class:"button_forum"}),(0,i.kt)("a",{href:"https://www.seeedstudio.com/contacts",class:"button_email"})),(0,i.kt)("div",{class:"button_tech_support_container"},(0,i.kt)("a",{href:"https://discord.gg/eWkprNDMU7",class:"button_discord"}),(0,i.kt)("a",{href:"https://github.com/Seeed-Studio/wiki-documents/discussions/69",class:"button_discussion"})))}d.isMDXComponent=!0}}]);