"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[69342],{15680:(e,i,n)=>{n.d(i,{xA:()=>c,yg:()=>f});var t=n(96540);function o(e,i,n){return i in e?Object.defineProperty(e,i,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[i]=n,e}function r(e,i){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);i&&(t=t.filter((function(i){return Object.getOwnPropertyDescriptor(e,i).enumerable}))),n.push.apply(n,t)}return n}function a(e){for(var i=1;i<arguments.length;i++){var n=null!=arguments[i]?arguments[i]:{};i%2?r(Object(n),!0).forEach((function(i){o(e,i,n[i])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(i){Object.defineProperty(e,i,Object.getOwnPropertyDescriptor(n,i))}))}return e}function s(e,i){if(null==e)return{};var n,t,o=function(e,i){if(null==e)return{};var n,t,o={},r=Object.keys(e);for(t=0;t<r.length;t++)n=r[t],i.indexOf(n)>=0||(o[n]=e[n]);return o}(e,i);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)n=r[t],i.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var d=t.createContext({}),l=function(e){var i=t.useContext(d),n=i;return e&&(n="function"==typeof e?e(i):a(a({},i),e)),n},c=function(e){var i=l(e.components);return t.createElement(d.Provider,{value:i},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var i=e.children;return t.createElement(t.Fragment,{},i)}},m=t.forwardRef((function(e,i){var n=e.components,o=e.mdxType,r=e.originalType,d=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=l(n),m=o,f=u["".concat(d,".").concat(m)]||u[m]||p[m]||r;return n?t.createElement(f,a(a({ref:i},c),{},{components:n})):t.createElement(f,a({ref:i},c))}));function f(e,i){var n=arguments,o=i&&i.mdxType;if("string"==typeof e||o){var r=n.length,a=new Array(r);a[0]=m;var s={};for(var d in i)hasOwnProperty.call(i,d)&&(s[d]=i[d]);s.originalType=e,s[u]="string"==typeof e?e:o,a[1]=s;for(var l=2;l<r;l++)a[l]=n[l];return t.createElement.apply(null,a)}return t.createElement.apply(null,n)}m.displayName="MDXCreateElement"},17363:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>d,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var t=n(58168),o=(n(96540),n(15680));const r={description:"TinyML on Seeed Studio XIAO SAMD21",title:"TinyML on Seeed Studio XIAO SAMD21",keywords:["xiao"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/Seeeduino-XIAO-TinyML",last_update:{date:"1/11/2023",author:"shuxu hu"}},a="TinyML on Seeed Studio XIAO Series",s={unversionedId:"Sensor/SeeedStudio_XIAO/SeeedStudio_XIAO_SAMD21/Embedded ML/Seeeduino-XIAO-TinyML",id:"Sensor/SeeedStudio_XIAO/SeeedStudio_XIAO_SAMD21/Embedded ML/Seeeduino-XIAO-TinyML",title:"TinyML on Seeed Studio XIAO SAMD21",description:"TinyML on Seeed Studio XIAO SAMD21",source:"@site/docs/Sensor/SeeedStudio_XIAO/SeeedStudio_XIAO_SAMD21/Embedded ML/Seeeduino-XIAO-TinyML.md",sourceDirName:"Sensor/SeeedStudio_XIAO/SeeedStudio_XIAO_SAMD21/Embedded ML",slug:"/Seeeduino-XIAO-TinyML",permalink:"/Seeeduino-XIAO-TinyML",draft:!1,editUrl:"https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/Sensor/SeeedStudio_XIAO/SeeedStudio_XIAO_SAMD21/Embedded ML/Seeeduino-XIAO-TinyML.md",tags:[],version:"current",lastUpdatedBy:"shuxu hu",lastUpdatedAt:1673395200,formattedLastUpdatedAt:"Jan 11, 2023",frontMatter:{description:"TinyML on Seeed Studio XIAO SAMD21",title:"TinyML on Seeed Studio XIAO SAMD21",keywords:["xiao"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/Seeeduino-XIAO-TinyML",last_update:{date:"1/11/2023",author:"shuxu hu"}},sidebar:"ProductSidebar",previous:{title:"MicroPython",permalink:"/XIAO-SAMD21-MicroPython"},next:{title:"Seeed Studio XIAO SAMD21 with TinyUSB",permalink:"/Seeeduino-XIAO-TinyUSB"}},d={},l=[{value:"Data acquisition and model training",id:"data-acquisition-and-model-training",level:2},{value:"Model deployment",id:"model-deployment",level:2},{value:"Reference",id:"reference",level:2}],c={toc:l},u="wrapper";function p(e){let{components:i,...n}=e;return(0,o.yg)(u,(0,t.A)({},c,n,{components:i,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"tinyml-on-seeed-studio-xiao-series"},"TinyML on Seeed Studio XIAO Series"),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:400,src:"https://files.seeedstudio.com/wiki/Wio-Terminal-TinyML-EI-1/Seeeduino-XIAO-pinout.jpg"})),(0,o.yg)("p",null,"Thanks to recent improvements in model optimization and emergence of frameworks specifically created for running machine learning model inference on microcontrollers, it has became possible to give more intelligence to these tiny devices. We now can deploy neural networks on microcontrollers for audio scene recognition (for example elephant activity or sound of breaking glass), hot-word detection(to activate device with a specific phrase) or even for simple image recognition tasks. The devices with embedded microcontrollers can be used to give new life and meaning to old sensors, such as using an accelerometer installed on a mechanism for anomaly detection and predictive maintenance \u2013 or to distinguish various kinds of liqueurs as in ",(0,o.yg)("a",{parentName:"p",href:"https://wiki.seeedstudio.com/Wio-Terminal-Edge-Impulse-Distinguish-Alochol/"},"this demo"),"! "),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{src:"https://files.seeedstudio.com/wiki/Wio-Terminal-Edge-Impulse/booze.jpg"})),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"The possibilities of TinyML are truly huge.")),(0,o.yg)("p",null,"We have made a ",(0,o.yg)("a",{parentName:"p",href:"https://wiki.seeedstudio.com/Wio-Terminal-TinyML/"},"whole series on deploying tiny machine learning models")," to another Seeed studio product, a compact development board in a plastic case, Wio Terminal. But it is possible to go tinier and deploy similar models to ARM Cortex M0+ and the little ",(0,o.yg)("a",{parentName:"p",href:"https://www.seeedstudio.com/Seeeduino-XIAO-Arduino-Microcontroller-SAMD21-Cortex-M0+-p-4426.html"},"Seeed Studio XIAO SAMD21")," board which is built around it \u2013 the board is as small as a thumb(20\xd717.5mm), consumes only 1.33 mAh of power (which means it can work ~112 hours on a 150 mA battery, much more if put in deep sleep) and cost as little as 4.3 USD."),(0,o.yg)("p",null,"This project covers training and deploying model to Seeed Studio XIAO SAMD21 and Seeed Studio XIAO RP2040 development boards. For additional information, have a look at the corresponding video!"),(0,o.yg)("iframe",{width:560,height:315,src:"https://www.youtube.com/embed/04_7U8MzVKg",frameBorder:0,allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0}),(0,o.yg)("h2",{id:"data-acquisition-and-model-training"},"Data acquisition and model training"),(0,o.yg)("p",null,"Software engineers spend a lot of time in front of the glowing screen on my chair. And later in the day it becomes difficult to maintain a proper pose. If only there was a way to make a device that could learn your specific body position for proper and wrong poses and warn you when you slouch too much or go into \u201cPython pose\u201d\u2026 Wait a moment, there is!"),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{src:"https://files.seeedstudio.com/wiki/Seeeduino-XIAO/img/utxkrcg5yss61.png"})),(0,o.yg)("p",null,"The best sensor for the task that will provide the data for machine learning model is obviously accelerometer. The original Seeed Studio XIAO SAMD21 and Seeed Studio XIAO RP2040, being very small do not come equipped with accelerometer sensor, while newer Seeed Studio XIAO nRF52840 Sense comes with built-in accelerometer. "),(0,o.yg)("p",null,"If you use original Seeed Studio XIAO SAMD21 and Seeed Studio XIAO RP2040, you can connect ",(0,o.yg)("a",{parentName:"p",href:"https://wiki.seeedstudio.com/Grove-3-Axis-Digital-Accelerometer-LIS3DHTR/"},"Grove LIS3DH accelerometer")," module to ",(0,o.yg)("a",{parentName:"p",href:"https://www.seeedstudio.com/Seeeduino-XIAO-Expansion-board-p-4746.html"},"Seeed Studio XIAO expansion board")," and start collecting the data. Collect 3 data samples for each posture, 60 seconds each with device attached to a t-shirt on your back."),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{src:"https://files.seeedstudio.com/wiki/Seeeduino-XIAO/img/image-31.png"})),(0,o.yg)("p",null,"For each sample, maintain the same pose, but include some arm, head and torso movements to simulate normal activity."),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{src:"https://files.seeedstudio.com/wiki/Seeeduino-XIAO/img/image-32.png"})),(0,o.yg)("p",null,"Choose 5 seconds time window with window shift of 1 second and Flatten processing block, since we are dealing with very slow moving data. A very plain fully connected network provided a good accuracy. In Reference section at the bottom of the article, you can find link to public version of the Edge Impulse project."),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{src:"https://files.seeedstudio.com/wiki/Seeeduino-XIAO/img/image-33.png"})),(0,o.yg)("p",null,"Some improvement can be made by collecting more data and making sure proper and improper postures can be recognized with some variations in device positioning on the clothes. Since the device is thought to be individual usage device it does not need to generalize to different people\u2019s postures and can be easily re-trained. You can check how well it detects your postures after training in Live classification tab."),(0,o.yg)("h2",{id:"model-deployment"},"Model deployment"),(0,o.yg)("p",null,"After you\u2019re satisfied with accuracy download the resulting model as Arduino library and copy it to your Arduino sketches/libraries folder. You can find sample code in the Reference section at the bottom of the article. The sample code collects 5 second sample, performs the inference and turns on the buzzer if one of the improper poses is detected."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-cpp"},'void loop()\n{\n\n    ei_printf("Sampling...\\n");\n\n    // Allocate a buffer here for the values we\'ll read from the IMU\n    float buffer[EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE] = { 0 };\n\n    for (size_t ix = 0; ix < EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE; ix += 3) {\n        // Determine the next tick (and then sleep later)\n        uint64_t next_tick = micros() + (EI_CLASSIFIER_INTERVAL_MS * 1000);\n\n        lis.getAcceleration(&buffer[ix], &buffer[ix+1], &buffer[ix + 2]);\n        buffer[ix + 0] *= CONVERT_G_TO_MS2;\n        buffer[ix + 1] *= CONVERT_G_TO_MS2;\n        buffer[ix + 2] *= CONVERT_G_TO_MS2;\n\n        delayMicroseconds(next_tick - micros());\n    }\n\n    // Turn the raw buffer in a signal which we can the classify\n    signal_t signal;\n    int err = numpy::signal_from_buffer(buffer, EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE, &signal);\n    if (err != 0) {\n        ei_printf("Failed to create signal from buffer (%d)\\n", err);\n        return;\n    }\n\n    // Run the classifier\n    ei_impulse_result_t result = { 0 };\n\n    err = run_classifier(&signal, &result, debug_nn);\n    if (err != EI_IMPULSE_OK) {\n        ei_printf("ERR: Failed to run classifier (%d)\\n", err);\n        return;\n    }\n\n    // print the predictions\n    ei_printf("Predictions ");\n    ei_printf("(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)",\n        result.timing.dsp, result.timing.classification, result.timing.anomaly);\n    ei_printf(": \\n");\n    for (size_t ix = 0; ix < EI_CLASSIFIER_LABEL_COUNT; ix++) {\n        ei_printf("    %s: %.5f\\n", result.classification[ix].label, result.classification[ix].value);\n    }\n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n    ei_printf("    anomaly score: %.3f\\n", result.anomaly);\n#endif\n    \n  if (result.classification[1].value > ALARM_THRESHOLD || result.classification[2].value > ALARM_THRESHOLD)\n  {     \n  tone(BUZZER_PIN, 523, 250);\n  delay(250);\n  noTone(BUZZER_PIN);\n  delay(250);  \n  tone(BUZZER_PIN, 523, 250);\n  delay(250);  \n  noTone(BUZZER_PIN);    \n  }\n\n}\n')),(0,o.yg)("p",null,"Since it is relatively slowly changing data and we do not need fast response times, normal sequential inference pipeline suits this application well."),(0,o.yg)("p",null,"A step above would be to use the newest Seeed Studio XIAO nRF52840 and connect the device to user\u2019s smartphone, which would allow for better alerts, statistics and so on."),(0,o.yg)("p",null,"Happy tinkering and remember to keep good posture!"),(0,o.yg)("h2",{id:"reference"},"Reference"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},(0,o.yg)("a",{parentName:"p",href:"https://studio.edgeimpulse.com/public/20025/latest"},"Edge Impulse Public project"))),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("p",{parentName:"li"},(0,o.yg)("a",{parentName:"p",href:"https://github.com/Seeed-Studio/Seeed_Arduino_Sketchbook/tree/master/examples/SeeeduinoXIAO_TinyML_7_Posture_Detection"},"Project Github")))))}p.isMDXComponent=!0}}]);