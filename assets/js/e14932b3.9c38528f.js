"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[44502],{15680:(e,t,n)=>{n.d(t,{xA:()=>d,yg:()=>y});var o=n(96540);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var p=o.createContext({}),l=function(e){var t=o.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},d=function(e){var t=l(e.components);return o.createElement(p.Provider,{value:t},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},u=o.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,p=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),c=l(n),u=r,y=c["".concat(p,".").concat(u)]||c[u]||m[u]||i;return n?o.createElement(y,a(a({ref:t},d),{},{components:n})):o.createElement(y,a({ref:t},d))}));function y(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,a=new Array(i);a[0]=u;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s[c]="string"==typeof e?e:r,a[1]=s;for(var l=2;l<i;l++)a[l]=n[l];return o.createElement.apply(null,a)}return o.createElement.apply(null,n)}u.displayName="MDXCreateElement"},86895:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>a,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>l});var o=n(58168),r=(n(96540),n(15680));const i={description:"Export For Model Assistant",title:"Export",keywords:["sscma model assistant ai tinyml"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/ModelAssistant_Tutorials_Export_Overview",last_update:{date:"01/11/2024",author:"LynnL4"}},a="Model Export",s={unversionedId:"Topics/TinyML/ModelAssistant/tutorials/export/overview",id:"Topics/TinyML/ModelAssistant/tutorials/export/overview",title:"Export",description:"Export For Model Assistant",source:"@site/docs/Topics/TinyML/ModelAssistant/tutorials/export/overview.md",sourceDirName:"Topics/TinyML/ModelAssistant/tutorials/export",slug:"/ModelAssistant_Tutorials_Export_Overview",permalink:"/ModelAssistant_Tutorials_Export_Overview",draft:!1,editUrl:"https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/Topics/TinyML/ModelAssistant/tutorials/export/overview.md",tags:[],version:"current",lastUpdatedBy:"LynnL4",lastUpdatedAt:1704931200,formattedLastUpdatedAt:"Jan 11, 2024",frontMatter:{description:"Export For Model Assistant",title:"Export",keywords:["sscma model assistant ai tinyml"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/ModelAssistant_Tutorials_Export_Overview",last_update:{date:"01/11/2024",author:"LynnL4"}},sidebar:"ProductSidebar",previous:{title:"Keypoint Detecion - PFLD",permalink:"/ModelAssistant_Tutorials_Training_PFLD"},next:{title:"PyTorch to ONNX",permalink:"/ModelAssistant_Tutorials_Export_PyTorch_2_ONNX"}},p={},l=[{value:"Parameter Descriptions",id:"parameter-descriptions",level:2}],d={toc:l},c="wrapper";function m(e){let{components:t,...n}=e;return(0,r.yg)(c,(0,o.A)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"model-export"},"Model Export"),(0,r.yg)("p",null,(0,r.yg)("a",{parentName:"p",href:"https://github.com/Seeed-Studio/ModelAssistant"},"SSCMA")," currently supports the following methods to convert and export models.\nYou can refer to the corresponding tutorials to complete the model export, and\nthen put the exported model into deployment."),(0,r.yg)("admonition",{type:"tip"},(0,r.yg)("p",{parentName:"admonition"},"By default, both ONNX and TFLite models are exported. If you only need to\nexport one of them, you can use the ",(0,r.yg)("inlineCode",{parentName:"p"},"--targets")," parameter to specify the\nexported model type, e.g. ",(0,r.yg)("inlineCode",{parentName:"p"},"--targets onnx")," or ",(0,r.yg)("inlineCode",{parentName:"p"},"--targets tflite"),".")),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("a",{parentName:"p",href:"/ModelAssistant_Tutorials_Export_PyTorch_2_ONNX"},"PyTorch to ONNX"),": Converts PyTorch model and ",(0,r.yg)("inlineCode",{parentName:"p"},".pth"),"\nweights to ONNX model ",(0,r.yg)("inlineCode",{parentName:"p"},".onnx"))),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("a",{parentName:"p",href:"/ModelAssistant_Tutorials_Export_PyTorch_2_TFlite"},"PyTorch to TFLite"),": Converts PyTorch model and ",(0,r.yg)("inlineCode",{parentName:"p"},".pth"),"\nweights to TFLite model ",(0,r.yg)("inlineCode",{parentName:"p"},".tflite")))),(0,r.yg)("admonition",{type:"tip"},(0,r.yg)("p",{parentName:"admonition"},"Before you can start exporting models, you need to complete the\n",(0,r.yg)("a",{parentName:"p",href:"/ModelAssistant_Tutorials_Training_Overview"},"Training")," section and obtain model weights ",(0,r.yg)("inlineCode",{parentName:"p"},".pth")," file\nbefore start exporting.")),(0,r.yg)("h2",{id:"parameter-descriptions"},"Parameter Descriptions"),(0,r.yg)("p",null,"For more parameters for model exporting, you can refer the code below."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sh"},"python3 tools/export.py --help\n\n# Convert and export PyTorch model to TFLite or ONNX models\n\n# positional arguments:\n#   config                the model config file path\n#   checkpoint            the PyTorch checkpoint file path\n\n# optional arguments:\n#   -h, --help            show this help message and exit\n#   --targets TARGETS [TARGETS ...]\n#                         the target type of model(s) to export e.g. tflite onnx\n#   --precisions PRECISIONS [PRECISIONS ...]\n#                         the precisions exported model, e.g. 'int8', 'uint8', 'int16', 'float16' and 'float32'\n#   --work_dir WORK_DIR, --work-dir WORK_DIR\n#                         the directory to save logs and models\n#   --output_stem OUTPUT_STEM, --output-stem OUTPUT_STEM\n#                         the stem of output file name (with path)\n#   --device DEVICE       the device used for convert & export\n#   --input_shape INPUT_SHAPE [INPUT_SHAPE ...], --input-shape INPUT_SHAPE [INPUT_SHAPE ...]\n#                         the shape of input data, e.g. 1 3 224 224\n#   --input_type {audio,image,sensor}, --input-type {audio,image,sensor}\n#                         the type of input data\n#   --cfg_options CFG_OPTIONS [CFG_OPTIONS ...], --cfg-options CFG_OPTIONS [CFG_OPTIONS ...]\n#                         override some settings in the used config, the key-value pair in 'xxx=yyy' format will be merged into config file\n#   --simplify SIMPLIFY   the level of graph simplification, 0 means disable, max: 5\n#   --opset_version OPSET_VERSION, --opset-version OPSET_VERSION\n#                         ONNX: operator set version of exported model\n#   --dynamic_export, --dynamic-export\n#                         ONNX: export with a dynamic input shape\n#   --algorithm {l2,kl}   TFLite: conversion algorithm\n#   --backend {qnnpack,fbgemm}\n#                         TFLite: converter backend\n#   --calibration_epochs CALIBRATION_EPOCHS, --calibration-epochs CALIBRATION_EPOCHS\n#                         TFLite: max epoches for quantization calibration\n#   --mean MEAN [MEAN ...]\n#                         TFLite: mean for model input (quantization), range: [0, 1], applied to all channels, using the average if multiple values are provided\n#   --mean_and_std MEAN_AND_STD [MEAN_AND_STD ...], --mean-and-std MEAN_AND_STD [MEAN_AND_STD ...]\n#                         TFLite: mean and std for model input(s), default: [((0.0,), (1.0,))], calculated on normalized input(s), applied to all channel(s), using the average if multiple values are provided\n")))}m.isMDXComponent=!0}}]);