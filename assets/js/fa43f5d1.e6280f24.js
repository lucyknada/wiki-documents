"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[75369],{15680:(e,t,n)=>{n.d(t,{xA:()=>p,yg:()=>m});var a=n(96540);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),d=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},p=function(e){var t=d(e.components);return a.createElement(l.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},g=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),c=d(n),g=o,m=c["".concat(l,".").concat(g)]||c[g]||u[g]||i;return n?a.createElement(m,r(r({ref:t},p),{},{components:n})):a.createElement(m,r({ref:t},p))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=g;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[c]="string"==typeof e?e:o,r[1]=s;for(var d=2;d<i;d++)r[d]=n[d];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}g.displayName="MDXCreateElement"},23490:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var a=n(58168),o=(n(96540),n(15680));const i={description:"Complete YOLOv8 Model Training on reComputer",title:"How to train and deploy YOLOv8 on reComputer",keywords:["reComputer","Train YOLOv8"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/How_to_Train_and_Deploy_YOLOv8_on_reComputer",last_update:{date:"12/6/2023",author:"Youjiang"}},r="How to Train and Deploy YOLOv8 on reComputer",s={unversionedId:"Edge/NVIDIA_Jetson/Application/Computer_Vision/How_to_Train_and_Deploy_YOLOv8_on_reComputer",id:"Edge/NVIDIA_Jetson/Application/Computer_Vision/How_to_Train_and_Deploy_YOLOv8_on_reComputer",title:"How to train and deploy YOLOv8 on reComputer",description:"Complete YOLOv8 Model Training on reComputer",source:"@site/docs/Edge/NVIDIA_Jetson/Application/Computer_Vision/How_to_Train_and_Deploy_YOLOv8_on_reComputer.md",sourceDirName:"Edge/NVIDIA_Jetson/Application/Computer_Vision",slug:"/How_to_Train_and_Deploy_YOLOv8_on_reComputer",permalink:"/How_to_Train_and_Deploy_YOLOv8_on_reComputer",draft:!1,editUrl:"https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/Edge/NVIDIA_Jetson/Application/Computer_Vision/How_to_Train_and_Deploy_YOLOv8_on_reComputer.md",tags:[],version:"current",lastUpdatedBy:"Youjiang",lastUpdatedAt:1701820800,formattedLastUpdatedAt:"Dec 6, 2023",frontMatter:{description:"Complete YOLOv8 Model Training on reComputer",title:"How to train and deploy YOLOv8 on reComputer",keywords:["reComputer","Train YOLOv8"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/How_to_Train_and_Deploy_YOLOv8_on_reComputer",last_update:{date:"12/6/2023",author:"Youjiang"}},sidebar:"ProductSidebar",previous:{title:"DashCamNet with Jetson Xavier NX Multicamera",permalink:"/DashCamNet-with-Jetson-Xavier-NX-Multicamera"},next:{title:"MaskCam",permalink:"/Jetson-Nano-MaskCam"}},l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Dataset",id:"dataset",level:2},{value:"Download public datasets",id:"download-public-datasets",level:3},{value:"Collecting and annotating data",id:"collecting-and-annotating-data",level:3},{value:"Model",id:"model",level:2},{value:"Train",id:"train",level:2},{value:"Validation",id:"validation",level:2},{value:"Deployment",id:"deployment",level:2},{value:"Summary",id:"summary",level:2},{value:"Tech Support &amp; Product Discussion",id:"tech-support--product-discussion",level:2}],p={toc:d},c="wrapper";function u(e){let{components:t,...n}=e;return(0,o.yg)(c,(0,a.A)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"how-to-train-and-deploy-yolov8-on-recomputer"},"How to Train and Deploy YOLOv8 on reComputer"),(0,o.yg)("h2",{id:"introduction"},"Introduction"),(0,o.yg)("p",null,"In the face of increasingly complex and dynamic challenges, the application of artificial intelligence provides new avenues for solving problems and has made significant contributions to the sustainable development of global society and the improvement of people's quality of life. Typically, before deploying artificial intelligence algorithms, the design and training of AI models take place on high-performance computing servers. Once the model training is complete, it is exported to edge computing devices for edge inference. In fact, all these processes can occur directly on edge computing devices. Specifically, tasks such as preparing datasets, training neural networks, validating neural networks, and deploying models can be performed on edge devices. This not only ensures data security but also saves costs associated with purchasing additional devices."),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_J4012.png"})),(0,o.yg)("div",{class:"get_one_now_container",style:{textAlign:"center"}},(0,o.yg)("a",{class:"get_one_now_item",href:"https://www.seeedstudio.com/reComputer-J4012-p-5586.html?queryID=3d7dba9378be2accafeaff54420edb6a&objectID=5586&indexName=bazaar_retailer_products"},(0,o.yg)("strong",null,(0,o.yg)("span",null,(0,o.yg)("font",{color:"FFFFFF",size:"4"}," Get One Now \ud83d\uddb1\ufe0f"))))),(0,o.yg)("p",null,"In this document, we train and deploy a object detection model for traffic scenes on the\n",(0,o.yg)("a",{parentName:"p",href:"https://www.seeedstudio.com/reComputer-J4012-p-5586.html?queryID=f6de8f6c8d814c021e13f4455d041d03&objectID=5586&indexName=bazaar_retailer_products"},"reComputer J4012"),".\nThis document uses the\n",(0,o.yg)("a",{parentName:"p",href:"https://www.ultralytics.com/"},"YOLOv8"),"\nobject detection algorithm as an example and provides a detailed overview of the entire process. Please note that all the operations described below take place on the Jetson edge computing device, ensuring that the Jetson device has an operating system installed that is\n",(0,o.yg)("a",{parentName:"p",href:"https://wiki.seeedstudio.com/NVIDIA_Jetson/"},"JetPack 5.0"),"\nor above."),(0,o.yg)("h2",{id:"dataset"},"Dataset"),(0,o.yg)("p",null,"The process of machine learning involves finding patterns within given data and then using a function to capture these patterns. Therefore, the quality of the dataset directly affects the performance of the model. Generally speaking, the better the quality and quantity of training data, the better the model trained. Therefore, the preparation of the dataset is crucial."),(0,o.yg)("p",null,"There are various methods for collecting training dataset. Here, two methods are introduced: 1. Download pre-annotated open-source public datasets. 2. Collect and annotate training data. Finally, consolidate all the data to prepare for the subsequent training phase."),(0,o.yg)("h3",{id:"download-public-datasets"},"Download public datasets"),(0,o.yg)("p",null,"Public datasets are shared standardized data resources widely used in machine learning and artificial intelligence research. They provide researchers with standard benchmarks to evaluate algorithm performance, fostering innovation and collaboration in the field. These datasets drive the AI community towards a more open, innovative, and sustainable direction."),(0,o.yg)("p",null,"There are many platforms where you can freely download datasets, such as\n",(0,o.yg)("a",{parentName:"p",href:"https://roboflow.com/"},"Roboflow"),",\n",(0,o.yg)("a",{parentName:"p",href:"https://www.kaggle.com/"},"Kaggle"),",\nand more. Here, we download an annotated dataset related to traffic scenes,\n",(0,o.yg)("a",{parentName:"p",href:"https://www.kaggle.com/datasets/yusufberksardoan/traffic-detection-project/download?datasetVersionNumber=1"},"Traffic Detection Project"),",\nfrom Kaggle."),(0,o.yg)("p",null,"The file structure after extraction is as follows:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-sh"},"archive\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 README.dataset.txt\n\u251c\u2500\u2500 README.roboflow.txt\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 images\n\u2502   \u2502   \u251c\u2500\u2500 aguanambi-1000_png_jpg.rf.7179a0df58ad6448028bc5bc21dca41e.jpg\n\u2502   \u2502   \u251c\u2500\u2500 aguanambi-1095_png_jpg.rf.4d9f0370f1c09fb2a1d1666b155911e3.jpg\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels\n\u2502       \u251c\u2500\u2500 aguanambi-1000_png_jpg.rf.7179a0df58ad6448028bc5bc21dca41e.txt\n\u2502       \u251c\u2500\u2500 aguanambi-1095_png_jpg.rf.4d9f0370f1c09fb2a1d1666b155911e3.txt\n\u2502       \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 train\n\u2502   \u251c\u2500\u2500 images\n\u2502   \u2502   \u251c\u2500\u2500 aguanambi-1000_png_jpg.rf.0ab6f274892b9b370e6441886b2d7b9d.jpg\n\u2502   \u2502   \u251c\u2500\u2500 aguanambi-1000_png_jpg.rf.dc59d3c5df5d991c1475e5957ea9948c.jpg\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels\n\u2502       \u251c\u2500\u2500 aguanambi-1000_png_jpg.rf.0ab6f274892b9b370e6441886b2d7b9d.txt\n\u2502       \u251c\u2500\u2500 aguanambi-1000_png_jpg.rf.dc59d3c5df5d991c1475e5957ea9948c.txt\n\u2502       \u251c\u2500\u2500 ...\n\u2514\u2500\u2500 valid\n    \u251c\u2500\u2500 images\n    \u2502   \u251c\u2500\u2500 aguanambi-1085_png_jpg.rf.0608a42a5c9090a4efaf9567f80fa992.jpg\n    \u2502   \u251c\u2500\u2500 aguanambi-1105_png_jpg.rf.0aa6c5d1769ce60a33d7b51247f2a627.jpg\n    \u2502   \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 labels\n        \u251c\u2500\u2500 aguanambi-1085_png_jpg.rf.0608a42a5c9090a4efaf9567f80fa992.txt\n        \u251c\u2500\u2500 aguanambi-1105_png_jpg.rf.0aa6c5d1769ce60a33d7b51247f2a627.txt\n        \u251c\u2500\u2500...\n")),(0,o.yg)("p",null,"Each image has a corresponding text file that contains the complete annotation information for that image. The ",(0,o.yg)("inlineCode",{parentName:"p"},"data.json")," file records the locations of the training, testing, and validation sets, and you need to modify the paths:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-json"},"train: ./train/images\nval: ./valid/images\ntest: ./test/images\n\nnc: 5\nnames: ['bicycle', 'bus', 'car', 'motorbike', 'person']\n")),(0,o.yg)("h3",{id:"collecting-and-annotating-data"},"Collecting and annotating data"),(0,o.yg)("p",null,"When public datasets cannot meet user requirements, neet to consider collecting and creating custom datasets tailored to specific needs. This can be achieved by collecting, annotating, and organizing relevant data.\nFor demonstration purposes, I captured and saved three images from\n",(0,o.yg)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=iJZcjZD0fw0"},"YouTube"),"\n, and try to use\n",(0,o.yg)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=iJZcjZD0fw0"},"Label Studio"),"\nto annotate the images."),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 1.")," Collect raw data:"),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/raw_datas.png"})),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 2.")," Install and run the annotation tool:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"sudo groupadd docker\nsudo gpasswd -a ${USER} docker\nsudo systemctl restart docker\nsudo chmod a+rw /var/run/docker.sock\n\nmkdir label_studio_data\nsudo chmod -R 776 label_studio_data\ndocker run -it -p 8080:8080 -v $(pwd)/label_studio_data:/label-studio/data heartexlabs/label-studio:latest\n")),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 3.")," Create a new project and complete the annotation as per the prompts:\n",(0,o.yg)("a",{parentName:"p",href:"https://labelstud.io/blog/quickly-create-datasets-for-training-yolo-object-detection-with-label-studio/#output-the-dataset-in-yolo-format"},"Label Studio Reference Documentation")),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/labeling.png"})),(0,o.yg)("p",null,"After completing the annotation, you can export the dataset in YOLO format and organize the annotated data along with the downloaded data. The simplest approach is to copy all the images to the train/images folder of the public dataset and the generated annotation text files to the train/labels folder of the public dataset."),(0,o.yg)("p",null,"At this point, we have obtained the training data through two different methods and integrated them. If you want higher-quality training data, there are many additional steps to consider, such as data cleaning, class balancing, and more. Since our task is relatively simple, we will skip these steps for now and proceed with training using the data obtained above."),(0,o.yg)("h2",{id:"model"},"Model"),(0,o.yg)("p",null,"In this section, we will download the YOLOv8 source code on reComputer and configure the runtime environment."),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 1.")," Use the following command to download the source code:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/ultralytics/ultralytics.git\ncd ultralytics\n")),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 2.")," Open requirements.txt and modify the relevant content:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"# Use the `vi` command to open the file\nvi requirements.txt\n\n# Press `a` to enter edit mode, and modify the following content:\ntorch>=1.7.0 --\x3e # torch>=1.7.0\ntorchvision>=0.8.1 --\x3e # torchvision>=0.8.1\n\n# Press `ESC` to exit edit mode, and finally input `:wq` to save and exit the file.\n\nStep 3. Run the following commands to download the required dependencies for YOLO and install YOLOv8:\npip3 install -e .\ncd ..\n")),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 4.")," Install the Jetson version of PyTorch:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"sudo apt-get install -y libopenblas-base libopenmpi-dev\nwget https://developer.download.nvidia.cn/compute/redist/jp/v512/pytorch/torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl -O torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl\npip3 install torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl\n")),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 5.")," Install the corresponding torchvision:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"sudo apt install -y libjpeg-dev zlib1g-dev\ngit clone --branch v0.16.0 https://github.com/pytorch/vision torchvision\ncd torchvision\npython3 setup.py install --user\ncd ..\n")),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 6.")," Use the following command to ensure that YOLO has been successfully installed:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"yolo detect predict model=yolov8s.pt source='https://ultralytics.com/images/bus.jpg'\n")),(0,o.yg)("h2",{id:"train"},"Train"),(0,o.yg)("p",null,"Model training is the process of updating model weights. By training the model, machine learning algorithms can learn from the data to recognize patterns and relationships, enabling predictions and decisions on new data."),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 1.")," Create a Python script for training:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"vi train.py\n")),(0,o.yg)("p",null,"Press ",(0,o.yg)("inlineCode",{parentName:"p"},"a")," to enter edit mode, and modify the following content:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO('yolov8s.pt')\n\n# Train the model\nresults = model.train(\n    data='/home/nvidia/Everything_Happens_Locally/Dataset/data.yaml', \n    batch=8, epochs=100, imgsz=640, save_period=5\n)\n")),(0,o.yg)("p",null,"Press ",(0,o.yg)("inlineCode",{parentName:"p"},"ESC")," to exit edit mode, and finally input ",(0,o.yg)("inlineCode",{parentName:"p"},":wq")," to save and exit the file.\nThe ",(0,o.yg)("inlineCode",{parentName:"p"},"YOLO.train()")," method has many configuration parameters; please refer to the\n",(0,o.yg)("a",{parentName:"p",href:"https://docs.ultralytics.com/modes/train/#arguments"},"documentation"),"\nfor details. Additionally, you can use a more streamlined ",(0,o.yg)("inlineCode",{parentName:"p"},"CLI")," approach to start training based on your specific requirements."),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 2.")," Start training with the following command:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"python3 train.py\n")),(0,o.yg)("p",null,"Then comes the lengthy waiting process. Considering the possibility of closing the remote connection window during the wait, this tutorial uses the\n",(0,o.yg)("a",{parentName:"p",href:"https://github.com/tmux/tmux/wiki"},"Tmux"),"\nterminal multiplexer. Thus, the interface I see during the process looks like this:"),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/training.png"})),(0,o.yg)("p",null,"Tmux is optional; as long as the model is training normally. After the training program finishes, you can find the model weight files saved during the training process in the designated folder:"),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/models.png"})),(0,o.yg)("h2",{id:"validation"},"Validation"),(0,o.yg)("p",null,"The validation process involves using a portion of the data to validate the reliability of the model. This process helps ensure that the model can perform tasks accurately and robustly in real-world applications. If you closely examine the information output during the training process, you'll notice that many validations are interspersed throughout the training. This section won't analyze the meaning of each evaluation metric but will instead analyze the model's usability by examining the prediction results."),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 1.")," Use the trained model to infer on a specific image:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"yolo detect predict \\\n    model='./runs/detect/train2/weights/best.pt' \\ \n    source='./datas/test/images/ant_sales-2615_png_jpg.rf.0ceaf2af2a89d4080000f35af44d1b03.jpg' \\\n    save=True show=False\n")),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/inference_cmd.png"})),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 2.")," Examine the inference results."),(0,o.yg)("p",null,"From the detection results, it can be observed that the trained model achieves the expected detection performance."),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/inference.jpeg"})),(0,o.yg)("h2",{id:"deployment"},"Deployment"),(0,o.yg)("p",null,"Deployment is the process of applying a trained machine learning or deep learning model to real-world scenarios. The content introduced above has validated the feasibility of the model, but it has not considered the inference efficiency of the model. In the deployment phase, it's necessary to find a balance between detection accuracy and efficiency. TensorRT inference engine can be used to improve the inference speed of the model."),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 1.")," To visually demonstrate the contrast between the lightweight and original models, create a new ",(0,o.yg)("inlineCode",{parentName:"p"},"inference.py")," file using the vi tool to implement video file inference. You can replace the inference model and input video by modifying lines 8 and 9. The input in this document is a video I shot with my phone."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"from ultralytics import YOLO\nimport os\nimport cv2\nimport time\nimport datetime\n\n\nmodel = YOLO(\"/home/nvidia/Everything_Happens_Locally/runs/detect/train2/weights/best.pt\")\ncap = cv2.VideoCapture('./sample_video.mp4')\n\nsave_dir = os.path.join('runs/inference_test', datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S'))\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nsize = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\noutput = cv2.VideoWriter(os.path.join(save_dir, 'result.mp4'), fourcc, fps, size)\n\nwhile cap.isOpened():\n    success, frame = cap.read()\n    if success:\n        start_time = time.time()\n        results = model(frame)\n        annotated_frame = results[0].plot()\n        total_time = time.time() - start_time\n        fps = 1/total_time\n        cv2.rectangle(annotated_frame, (20, 20), (200, 60), (55, 104, 0), -1)\n        cv2.putText(annotated_frame, f'FPS: {round(fps, 2)}', (30, 50), 0, 0.9, (255, 255, 255), thickness=2, lineType=cv2.LINE_AA)\n        print(f'FPS: {fps}')\n        cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n        output.write(annotated_frame)\n        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n            break\n    else:\n        break\n\ncv2.destroyAllWindows()\ncap.release()\noutput.release()\n")),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 2.")," Run the following command and record the inference speed before model quantization:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"python3 inference.py\n")),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/inference_pt.png"})),(0,o.yg)("p",null,"The result indicates that the inference speed of the model before quantization is 21.9 FPS"),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 3.")," Generate the quantized model:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"pip3 install onnx\nyolo export model=/home/nvidia/Everything_Happens_Locally/runs/detect/train2/weights/best.pt format=engine half=True device=0\n")),(0,o.yg)("p",null,"After the program to complete(about 10-20 minutes), a ",(0,o.yg)("inlineCode",{parentName:"p"},".engine")," file will be generated in the same directory as the input model. This file is the quantized model."),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/model_engine.png"})),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Step 4.")," Test the inference speed using the quantized model. "),(0,o.yg)("p",null,"Here, you need to modify the content of line 8 in the script created in Step 1."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"model = YOLO(<path to .pt>) --\x3e model = YOLO(<path to .engine>)\n")),(0,o.yg)("p",null,"Then, rerun the inference command:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"python3 inference.py\n")),(0,o.yg)("div",{align:"center"},(0,o.yg)("img",{width:800,src:"https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/inference_engine.png"})),(0,o.yg)("p",null,"From the perspective of inference efficiency, the quantized model shows a significant improvement in inference speed."),(0,o.yg)("h2",{id:"summary"},"Summary"),(0,o.yg)("p",null,"This article provides readers with a comprehensive guide that covers various aspects from data collection and model training to deployment. Importantly, all processes occur in reComputer, eliminating the need for additional GPUs from users."),(0,o.yg)("h2",{id:"tech-support--product-discussion"},"Tech Support & Product Discussion"),(0,o.yg)("p",null,"Thank you for choosing our products! We are here to provide you with different support to ensure that your experience with our products is as smooth as possible. We offer several communication channels to cater to different preferences and needs."),(0,o.yg)("div",{class:"button_tech_support_container"},(0,o.yg)("a",{href:"https://forum.seeedstudio.com/",class:"button_forum"}),(0,o.yg)("a",{href:"https://www.seeedstudio.com/contacts",class:"button_email"})),(0,o.yg)("div",{class:"button_tech_support_container"},(0,o.yg)("a",{href:"https://discord.gg/eWkprNDMU7",class:"button_discord"}),(0,o.yg)("a",{href:"https://github.com/Seeed-Studio/wiki-documents/discussions/69",class:"button_discussion"})))}u.isMDXComponent=!0}}]);